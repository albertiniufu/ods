\chapter{Grafos}
\chaplabel{graphs}

%\textbf{Warning to the Reader:} This chapter is still being actively
%developed, meaning that the code has not been thoroughly tested and/or
%the text has not be carefully proofread.

Neste capítulo, estudamos duas representações de grafos e algoritmos
básicos que usam essas representações.

Matematicamente, um \emph{grafo (direcionado)}
\index{grafo}%
\index{grafo direcionado}%
é um par
$G=(V,E)$ onde
$V$ é um conjunto de \emph{vértices}
\index{vértice}%
e $E$ é um conjunto de pares ordenados de vértices chamados de 
\emph{arestas}.
\index{aresta}%
Uma aresta #(i,j)# é \emph{direcionada}
\index{aresta direcionada}%
de #i# para #j#;  #i# é chamado de \emph{origem}
\index{origem}, ou \emph{source} em inglês, da aresta e #j#
é chamado de destino, ou \emph{target} em inglês.
\index{target}  Um caminho, \emph{path},%
\index{path} em $G$ é uma sequência de vértices 
$v_0,\ldots,v_k$ tal que, para todo $i\in\{1,\ldots,k\}$,
a aresta $(v_{i-1},v_{i})$ está em $E$.  Um caminho $v_0,\ldots,v_k$ é um
\emph{ciclo}
\index{ciclo}%
se, adicionalmente, a aresta
 $(v_k,v_0)$ está em $E$.  Um caminho (ou ciclo) é 
\emph{simples}
\index{caminho/ciclo simples}%
se todos o seus vértices são únicos. Se existir um caminho de algum vértice
$v_i$ para algum vértice $v_j$ então dizemos que 
$v_j$ é \emph{alcançável}
\index{vértice alcançável} de $v_i$.  Um exemplo de um grafo é mostrado na 
\figref{graph}.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/graph}
  \end{center}
  \caption{Um grafo com doze vértices. Vértices são desenhados como círculos numerados e arestas são desenhadas como curvas com setas apontando da origem ao destino.}
  \figlabel{graph}
\end{figure}

Devido à sua habilidade de modelar tantos fenômenos, grafos tem um número enorme de aplicações. Existem muitos exemplos óbvios. Redes de computadores podem
ser modeladas como grafos, com vértices correspondentes a computadores e arestas correspondentes a links (direcionados) de comunicação entre computadores.
Ruas de cidades podem ser modeladas como grafos, com vértices representando cruzamentos e arestas representando a ligação entre dois cruzamentos consecutivos.

Exemplos menos óbvios ocorrem assim que percebemos que grafos podem 
modelar qualquer relações que ocorrem em pares de elementos de um conjunto.
Por exemplo, em uma universidade podemos ter um \emph{grafo de conflitos} de horários
\index{grafo de conflitos} cujos vértices representam cursos oferecidos em uma 
universidade no qual a aresta #(i,j)# está presente se e somente se existe pelo 
menos um estudante que está frequentando a disciplina #i# e a disciplina #j#.
Então, uma aresta indica que a prova para a disciplina #i# não pode ser
agendada no mesmo horário que o exame para a disciplina #j#.

Ao longo dessa seção, iremos usar #n# para denotar o número de vértices
de $G$ e #m# para denotar o número de arestas de $G$. Isto é, 
$#n#=|V|$ e
$#m#=|E|$. Além disso, assumiremos que $V=\{0,\ldots,#n#-1\}$.
Outros dados que gostariamos de associar com os elementos de $V$
podem ser guardados em um array de tamanho $#n#$.

Algumas operações típicas realizadas em grafos são:
\begin{itemize}
  \item #addEdge(i,j)#: Adicionar a aresta $(#i#,#j#)$ a $E$.
  \item #removeEdge(i,j)#: Remover a aresta $(#i#,#j#)$ de $E$.
  \item #hasEdge(i,j)#: Verificar se a aresta $(#i#,#j#)\in E$ 
  \item #outEdges(i)#: Retornar uma #List# de todos os inteiros $#j#$ tais que
  $(#i#,#j#)\in E$
  \item #inEdges(i)#: Retornar uma #List# de todos os inteiros $#j#$ tais que 
  $(#j#,#i#)\in E$
\end{itemize}

Note que essas oepraçoes não são terrivelmente difíceis de implementar eficientemente. 
Por exemplo, as três primeiras operações podem ser implementadas diretamente 
usando um #USet#, de forma que podem ser implementadas em tempo constante esperado usando tabelas hash discutidas no \chapref{hashing}.
As duas últimas operações podem ser implementadas em tempo constante guardando, para cada vértice, uma lista de seu vértices adjacentes.

Entretanto, diferentes aplicações de grafos tem diferentes exigências de desempenho 
para essas operações e, idealmente, podemos usar a implementação 
mais simples que satisfaz todos os requisitos da aplicação.
Devido essa razão, nós discutimos duas grandes categorias de representações
de grafos.

\section{#AdjacencyMatrix#: Representando um Grafo com um Matriz}
\seclabel{adjacency-matrix}

\index{matriz de adjacências}%
Uma \emph{matriz de adjacências} é uma forma de representar um grafo com #n# vértices 
$G=(V,E)$ com uma matriz $#n#\times#n#$, #a#, cujas entradas são valores booleanos. 
\codeimport{ods/AdjacencyMatrix.a.n.AdjacencyMatrix(n0)}

A valor da matriz #a[i][j]# é definido como 
\[  #a[i][j]#= 
    \begin{cases}
      #true# & \text{se $#(i,j)#\in E$} \\
      #false# & \text{caso contrário}
    \end{cases}
\]

A matriz de adjacências para o grafo na \figref{graph} é mostrado na 
\figref{graph-adj}.

Nessa representação, as operações 
#addEdge(i,j)#,
#removeEdge(i,j)#, e #hasEdge(i,j)# somente atribuir e ler a entrada
da matriz
#a[i][j]#:
\codeimport{ods/AdjacencyMatrix.addEdge(i,j).removeEdge(i,j).hasEdge(i,j)}
Essas operações claramente levam tempo constante por operação. 

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/graph} \\[3ex]
    \begin{tabular}{c|cccccccccccc}
        &0&1&2&3&4&5&6&7&8&9&10&11 \\\hline
       0&0&1&0&0&1&0&0&0&0&0&0 &0\\
       1&1&0&1&0&0&1&1&0&0&0&0 &0\\
       2&1&0&0&1&0&0&1&0&0&0&0 &0\\
       3&0&0&1&0&0&0&0&1&0&0&0 &0\\
       4&1&0&0&0&0&1&0&0&1&0&0 &0\\
       5&0&1&1&0&1&0&1&0&0&1&0 &0\\
       6&0&0&1&0&0&1&0&1&0&0&1 &0\\
       7&0&0&0&1&0&0&1&0&0&0&0 &1\\
       8&0&0&0&0&1&0&0&0&0&1&0 &0\\
       9&0&0&0&0&0&1&0&0&1&0&1 &0\\
      10&0&0&0&0&0&0&1&0&0&1&0 &1\\
      11&0&0&0&0&0&0&0&1&0&0&1 &0\\
    \end{tabular} 
  \end{center}
  \caption{Um grafo e seu matriz de adjacências.}
  \figlabel{graph-adj}
\end{figure}

A matriz de adjacências tem desempenho ruim com as operações
#outEdges(i)# e 
#inEdges(i)#.  Para implementá-las, precisamos varrer todas as #n# entradas na correspondente linha ou coluna de #a# e reunir todos os índices #j# 
onde #a[i][j]#, respectivamente #a[j][i]#, seja true.
\javaimport{ods/AdjacencyMatrix.outEdges(i).inEdges(i)}
\cppimport{ods/AdjacencyMatrix.outEdges(i,edges).inEdges(i,edges)}
Essas operações claramente levam 
$O(#n#)$ de tempo por operação. 

Outra desvantagem da matriz de adjacências é o seu tamanho. Ela guarda
uma matriz booleana de tamanho $#n#\times #n#$, o que exige o uso de pelo 
menos $#n#^2$ bits de memória.   A implementação aqui usa uma matriz de 
valores \javaonly{#boolean#}\cpponly{#bool#} tal que ela na verdade usa na ordem de 
of $#n#^2$ bytes de memória.  Uma implementação mais cuidadosa, que empacota #w# valores booleanos em cada palavra de memória, poderia reduzir esse uso de espaço a 
$O(#n#^2/#w#)$ palavras de memória.

\begin{thm}
  A estrutura de dados #AdjacencyMatrix# implementa a interface #Graph#.
Uma #AdjacencyMatrix# aceita as operações 
\begin{itemize}
  \item #addEdge(i,j)#, #removeEdge(i,j)# e #hasEdge(i,j)# em tempo
    constante por operação; e 
  \item #inEdges(i)# e #outEdges(i)# em $O(#n#)$ tempo por operações.
\end{itemize}
O espaço usado por uma 
#AdjacencyMatrix# é $O(#n#^2)$.
\end{thm}

Apesar de alto uso de memória e desempenho ruim das operações 
 #inEdges(i)#
e #outEdges(i)#, uma #AdjacencyMatrix# pode ainda ser útil para
algumas aplicações. Em especial, quando o grafo $G$ é \emph{denso},
ele tem quase 
 $#n#^2$ arestas, então um uso de memória de $#n#^2$ pode
 ser aceitável. 

A estrutura de dados #AdjacencyMatrix# também é frequentemente usada
porque operações algébricas na matriz #a# podem ser usadas para computar
eficientemente as propriedades do grafo $G$. 
Isso é um tópico para uma disciplina sobre algoritmos, mas uma dessas
propriedades é: se tratarmos as entradas de #a# como inteiros (1 para #true# e 0 para
#false#) e multiplicarmos #a# por si mesma usando multiplicação de matrizes
então obtemos a matriz 
$#a#^2$.  Lembre-se, da definição de multiplicação de matrizes, que 
\[
    #a^2[i][j]# = \sum_{k=0}^{#n#-1} #a[i][k]#\cdot #a[k][j]# \enspace .
\]
Interpretando essa soma em termos do grafo $G$, essa fórmula conta o
número de vértices, 
$#k#$, tal que $G$ contém as arestas #(i,k)#
e #(k,j)#.  Isto é, isso conta o número de caminhos de $#i#$ a $#j#$
(por meio de vértices intermediários, $#k#$) cujo comprimento é exatamente dois.
Essa observação é a fundação de um algoritmo que computa os caminhos
mais curtos entre todos os pares de vértices em 
$G$ usando somente $O(\log
#n#)$ multiplicações de matrizes.

\section{#AdjacencyLists#: Um Grafo como uma Coleção de Listas}
\seclabel{adjacency-list}

\index{listas de adjacências}%
Representações de grafos com \emph{listas de adjacências} é uma abordagem centrada nos vértices.
Existem muitas implementações possíveis de listas de adjacências.
Nesta seção, apresentamos uma simples. No final da seção, discutimos
diferentes possibilidades. Em uma representação de listas de adjacências,
o grafo 
$G=(V,E)$ é representado como um array #adj# de listas. A lista 
#adj[i]# contém uma lista de vértices adjacentes ao vértice #i#.
Isto é, ela contém todo índice #j# tal que $#(i,j)#\in E$.
\codeimport{ods/AdjacencyLists.adj.n.AdjacencyLists(n0)}
(Uma exemplo é mostrado na \figref{graph-adjlist}.)  
Nessa implementação em especial, nós representamos cada lista em #adj# como 
\javaonly{uma}\cpponly{uma 
subclasse de} #ArrayStack#, porque gostariamos acesso pela posição em tempo constante.
Especificamente, poderíamos implementar 
#adj# como uma #DLList#.


\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/graph} \\[3ex]
    \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}\hline
        0&1&2&3&4&5&6 &7 &8&9 &10&11 \\\hline
        1&0&1&2&0&1&5 &6 &4&8 &9 &10 \\
        4&2&3&7&5&2&2 &3 &9&5 &6 &7 \\
         &6&6& &8&6&7 &11& &10&11& \\
         &5& & & &9&10&  & &  &  & \\
         & & & & &4&  &  & &  &  & \\
    \end{tabular} 
  \end{center}
  \caption{Uma grafo e suas listas de adjacência.}
  \figlabel{graph-adjlist}
\end{figure}

A operação
#addEdge(i,j)# simplesmente acrescenta o valor #j# no final da lista #adj[i]#:
\codeimport{ods/AdjacencyLists.addEdge(i,j)}
Isso leva tempo constante.

A operação #removeEdge(i,j)# busca ao longo da lista #adj[i]#
até que ache #j# e então o remove:
\codeimport{ods/AdjacencyLists.removeEdge(i,j)}
Isso leva $O(\deg(#i#))$ de tempo, onde $\deg(#i#)$ (o \emph{grau}
\index{grau}%
de
$#i#$) conta o número de arestas em $E$ que tem $#i#$ como origem.

A operação
#hasEdge(i,j)# é similar: ela busca ao longo da lista 
#adj[i]# até achar #j# (e retorna true), ou chega no final da lista (e retorna false): 
\codeimport{ods/AdjacencyLists.hasEdge(i,j)}
Isso também leva $O(\deg(#i#))$ de tempo.

A operação #outEdges(i)# é bem simples: 
\pcodeonly{retorna a lista #adj[i]#}
\javaonly{retorna a lista #adj[i]#}
\cpponly{copia os valores em #adj[i]# em uma lista de saída}:
\pcodeimport{ods/AdjacencyLists.outEdges(i)}
\javaimport{ods/AdjacencyLists.outEdges(i)}
\cppimport{ods/AdjacencyLists.outEdges(i,edges)}
\javaonly{Isso claramente leva tempo constante.}\cpponly{Isso leva $O(\deg(#i#))$ de tempo.}

A operação
#inEdges(i)# é muito mais trabalhosa. Ela varre todo vértice $j$ verificando se a aresta #(i,j)# existe e, caso positivo, adicionad #j# a uma lista de saída:
\pcodeimport{ods/AdjacencyLists.inEdges(i)}
\javaimport{ods/AdjacencyLists.inEdges(i)}
\cppimport{ods/AdjacencyLists.inEdges(i,edges)}
Essa operação é muito lenta. Ela verifica toda lista de adjacência de todos os vértices, então leva 
$O(#n# + #m#)$ de tempo.

O teorema a seguir resume o desempenho das estruturas de dados anteriormente descrita:

\begin{thm}
A estrutura de dados #AdjacencyLists# implementa a interface #Graph#.
Uma #AdjacencyLists# aceita as operações 
\begin{itemize}
  \item #addEdge(i,j)# em tempo constante por operação; 
  \item #removeEdge(i,j)# e #hasEdge(i,j)# em tempo $O(\deg(#i#))$ por operação; 
  \javaonly{\item #outEdges(i)# em tempo constante por operação; e}
  \cpponly{\item #outEdges(i)# em $O(\deg(#i#))$ de tempo por operação; e}
  \item #inEdges(i)# em $O(#n#+#m#)$ de tempo por operação.
\end{itemize}
O espaço usado por uma #AdjacencyLists# é $O(#n#+#m#)$.
\end{thm}

Conforme mencionado anteriormente, existem muitas diferentes escolhas que
podem ser feitas ao implementar um grafo como listas de adjacências. Algumas
escolhas incluem:
\begin{itemize}
  \item Que tipo de coleção deve ser usada para guardar cada elemento de 
  #adj#?  Podem ser usadas uma listas baseadas em arrays, listas ligadas ou mesmo tabelas hash.
  \item Deve haver uma segunda lista de adjacência, #inadj#, que guarda para cada #i#, a lista de vértices, #j#, tal que $#(j,i)#\in E$?
  Isso pode reduzir enormemente o tempo de execução da operação
  #inEdges(i)#, mas requer ligeiramente mais trabalho ao adicionar ou remover arestas. 
  %%%%%%%%%%%%%5
  \item Should the entry for the edge #(i,j)# in #adj[i]# be linked by
  a reference to the corresponding entry in #inadj[j]#?
  \item Should edges be first-class objects with their own associated data?
  In this way, #adj# would contain lists of edges rather than lists of vertices (integers).
\end{itemize}
Most of these questions come down to a tradeoff between complexity (and
space) of implementation and performance features of the implementation.

\section{Graph Traversal}

In this section we present two algorithms for exploring a graph,
starting at one of its vertices, #i#, and finding all vertices that
are reachable from #i#.  Both of these algorithms are best suited to
graphs represented using an adjacency list representation.  Therefore,
when analyzing these algorithms we will assume that the underlying
representation is an #AdjacencyLists#.

\subsection{Breadth-First Search}

\index{breadth-first-search}%
The \emph{breadth-first-search} algorithm starts at a vertex #i# and visits,
first the neighbours of #i#, then the neighbours of the neighbours of #i#,
then the neighbours of the neighbours of the neighbours of #i#, and so on.

This algorithm is a generalization of the breadth-first traversal
algorithm for binary trees (\secref{bintree:traversal}), and is
very similar; it uses a queue, #q#, that initially contains only #i#.
It then repeatedly extracts an element from #q# and adds its neighbours
to #q#, provided that these neighbours have never been in #q# before.
The only major difference between the breadth-first-search algorithm
for graphs and the one for trees is that the algorithm for graphs has
to ensure that it does not add the same vertex to #q# more than once.
It does this by using an auxiliary boolean array, #seen#, that tracks
which vertices have already been discovered.
\codeimport{ods/Algorithms.bfs(g,r)}
An example of running #bfs(g,0)# on the graph from \figref{graph}
is shown in \figref{graph-bfs}.  Different executions are possible,
depending on the ordering of the adjacency lists; \figref{graph-bfs}
uses the adjacency lists in \figref{graph-adjlist}.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/graph-bfs}
  \end{center}
  \caption[Breadth-first-search]{An example of breadth-first-search starting at node 0. Nodes are
  labelled with the order in which they are added to #q#.  Edges that
  result in nodes being added to #q# are drawn in black, other edges
  are drawn in grey.}
  \figlabel{graph-bfs}
\end{figure}

Analyzing the running-time of the #bfs(g,i)# routine is fairly
straightforward.  The use of the #seen# array ensures that no vertex is
added to #q# more than once.  Adding (and later removing) each vertex
from #q# takes constant time per vertex for a total of $O(#n#)$ time.
Since each vertex is processed by the inner loop at most once, each
adjacency list is processed at most once, so each edge of $G$ is processed
at most once.  This processing, which is done in the inner loop takes
constant time per iteration, for a total of $O(#m#)$ time.  Therefore,
the entire algorithm runs in $O(#n#+#m#)$ time.

The following theorem summarizes the performance of the #bfs(g,r)# algorithm.
\begin{thm}\thmlabel{bfs-graph}
  When given as input a #Graph#, #g#, that is implemented using the
  #AdjacencyLists# data structure, the #bfs(g,r)# algorithm runs in $O(#n#+#m#)$
  time.
\end{thm}

A breadth-first traversal has some very special properties.  Calling
#bfs(g,r)# will eventually enqueue (and eventually dequeue) every vertex
#j# such that there is a directed path from #r# to #j#.  Moreover,
the vertices at distance 0 from #r# (#r# itself) will enter #q# before
the vertices at distance 1, which will enter #q# before the vertices at
distance 2, and so on.  Thus, the #bfs(g,r)# method visits vertices
in increasing order of distance from #r# and vertices that cannot be
reached from #r# are never visited at all.

A particularly useful application of the breadth-first-search algorithm
is, therefore, in computing shortest paths.  To compute the shortest
path from #r# to every other vertex, we use a variant of #bfs(g,r)#
that uses an auxilliary array, #p#, of length #n#.  When a new vertex
#j# is added to #q#, we set #p[j]=i#.  In this way, #p[j]# becomes the
second last node on a shortest path from #r# to #j#.  Repeating this,
by taking #p[p[j]#, #p[p[p[j]]]#, and so on we can reconstruct the
(reversal of) a shortest path from #r# to #j#.



\subsection{Depth-First Search}

The \emph{depth-first-search}
\index{depth-first-search}%
algorithm is similar to the standard
algorithm for traversing binary trees;  it first fully explores one
subtree before returning to the current node and then exploring the
other subtree.  Another way to think of depth-first-search is by saying
that it is similar to breadth-first search except that it uses a stack
instead of a queue.

During the execution of the depth-first-search algorithm, each vertex,
#i#, is assigned a colour, #c[i]#: #white# if we have never seen
the vertex before, #grey# if we are currently visiting that vertex,
and #black# if we are done visiting that vertex.  The easiest way to
think of depth-first-search is as a recursive algorithm.  It starts by
visiting #r#.  When visiting a vertex #i#, we first mark #i# as #grey#.
Next, we scan #i#'s adjacency list and recursively visit any white vertex
we find in this list.  Finally, we are done processing #i#, so we colour
#i# black and return.
\codeimport{ods/Algorithms.dfs(g,r).dfs(g,i,c)}
An example of the execution of this algorithm is shown in \figref{graph-dfs}.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/graph-dfs}
  \end{center}
  \caption[Depth-first-search]{An example of depth-first-search starting at node 0. Nodes are
  labelled with the order in which they are processed.  Edges that
  result in a recursive call are drawn in black, other edges
  are drawn in #grey#.}
  \figlabel{graph-dfs}
\end{figure}

Although depth-first-search may best be thought of as a recursive
algorithm, recursion is not the best way to implement it. Indeed, the code
given above will fail for many large graphs by causing a stack overflow.
An alternative implementation is to replace the recursion stack with an
explicit stack, #s#.  The following implementation does just that:
\codeimport{ods/Algorithms.dfs2(g,r)} 
In the preceding code, when the next vertex, #i#, is processed, #i# is coloured
#grey# and then replaced, on the stack, with its adjacent vertices.
During the next iteration, one of these vertices will be visited.

Not surprisingly, the running times of #dfs(g,r)# and #dfs2(g,r)# are the
same as that of #bfs(g,r)#:
\begin{thm}\thmlabel{dfs-graph}
  When given as input a #Graph#, #g#, that is implemented using the
  #AdjacencyLists# data structure, the #dfs(g,r)# and #dfs2(g,r)# algorithms
  each run in $O(#n#+#m#)$ time.
\end{thm}

As with the breadth-first-search algorithm, there is an underlying
tree associated with each execution of depth-first-search.  When a node
$#i#\neq #r#$ goes from #white# to #grey#, this is because #dfs(g,i,c)#
was called recursively while processing some node #i'#.  (In the case
of #dfs2(g,r)# algorithm, #i# is one of the nodes that replaced #i'#
on the stack.)  If we think of #i'# as the parent of #i#, then we obtain
a tree rooted at #r#.  In \figref{graph-dfs}, this tree is a path from
vertex 0 to vertex 11.

An important property of the depth-first-search algorithm is the
following: Suppose that when node #i# is coloured #grey#, there exists a path
from #i# to some other node #j# that uses only white vertices.  Then #j#
will be coloured first #grey# then #black# before #i# is coloured #black#.
(This can be proven by contradiction, by considering any path $P$ from #i#
to #j#.)

One application of this property is the detection of cycles.
\index{cycle detection}%
Refer
to \figref{dfs-cycle}.  Consider some cycle, $C$, that can be reached
from #r#.  Let #i# be the first node of $C$ that is coloured #grey#,
and let #j# be the node that precedes #i# on the cycle $C$.  Then,
by the above property, #j# will be coloured #grey# and the edge #(j,i)#
will be considered by the algorithm while #i# is still #grey#.  Thus,
the algorithm can conclude that there is a path, $P$, from #i# to #j#
in the depth-first-search tree and the edge #(j,i)# exists.  Therefore,
$P$ is also a cycle.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dfs-cycle}
  \end{center}
  \caption[Cycle detection]{The depth-first-search algorithm can be used to detect cycles
  in $G$. The node #j# is coloured #grey# while #i# is still #grey#.  This
  implies that there is a path, $P$, from #i# to #j# in the depth-first-search
  tree, and the edge #(j,i)# implies that $P$ is also a cycle.}
  \figlabel{dfs-cycle}
\end{figure}

\section{Discussion and Exercises}

The running times of the depth-first-search and breadth-first-search
algorithms are somewhat overstated by the Theorems~\ref{thm:bfs-graph} and
\ref{thm:dfs-graph}.  Define $#n#_{#r#}$ as the number of vertices, #i#,
of $G$, for which there exists a path from #r# to #i#.  Define $#m#_#r#$
as the number of edges that have these vertices as their sources.
Then the following theorem is a more precise statement of the running
times of the breadth-first-search and depth-first-search algorithms.
(This more refined statement of the running time is useful in some of
the applications of these algorithms outlined in the exercises.)
\begin{thm}\thmlabel{graph-traversal}
  When given as input a #Graph#, #g#, that is implemented using the
  #AdjacencyLists# data structure, the #bfs(g,r)#, #dfs(g,r)# and #dfs2(g,r)#
  algorithms each run in $O(#n#_{#r#}+#m#_{#r#})$ time.
\end{thm}

Breadth-first search seems to have been discovered independently by
Moore \cite{m59} and Lee \cite{l61} in the contexts of maze exploration
and circuit routing, respectively.

Adjacency-list representations of graphs were presented by
Hopcroft and Tarjan \cite{ht73} as an alternative to the (then more
common) adjacency-matrix representation.  This representation, as well as
depth-first-search, played a major part in the celebrated Hopcroft-Tarjan
planarity testing algorithm 
\index{planarity testing}%
that can determine, in $O(#n#)$ time, if
a graph can be drawn, in the plane, and in such a way that no pair of
edges cross each other \cite{ht74}.

In the following exercises, an undirected graph is one in which, for
every #i# and #j#, the edge $(#i#,#j#)$ is present if and only if the
edge $(#j#,#i#)$ is present.
\index{undirected graph}%
\index{graph!undirected}%

\begin{exc}
  Draw an adjacency list representation and an adjacency matrix
  representation of the graph in \figref{graph-example2}.
\end{exc}

\begin{figure}
  \centering{\includegraphics[scale=0.90909]{figs/graph-example2}}
  \caption{An example graph.}
  \figlabel{graph-example2}
\end{figure}

\begin{exc}
  \index{incidence matrix}%
  The \emph{incidence matrix} representation of a graph,
  $G$, is an $#n#\times#m#$ matrix, $A$, where
  \[
     A_{i,j} = \begin{cases}
        -1 & \text{if vertex $i$ the source of edge $j$} \\
        +1 & \text{if vertex $i$ the target of edge $j$} \\
        0 & \text{otherwise.}
     \end{cases}
  \]
  \begin{enumerate}
    \item Draw the incident matrix representation of the graph in
      \figref{graph-example2}.
    \item Design, analyze and implement an incidence matrix representation
      of a graph.  Be sure to analyze the space, the cost of
      #addEdge(i,j)#, #removeEdge(i,j)#, #hasEdge(i,j)#, #inEdges(i)#,
      and #outEdges(i)#.
  \end{enumerate}
\end{exc}

\begin{exc}
  Illustrate an execution of the #bfs(G,0)# and #dfs(G,0)# on the graph, $#G#$,
  in \figref{graph-example2}.
\end{exc}

\begin{exc}
  \index{connected graph}%
  \index{graph!connected}%
  Let $G$ be an undirected graph.  We say $G$ is \emph{connected} if,
  for every pair of vertices #i# and #j# in $G$, there is a path from
  $#i#$ to $#j#$ (since $G$ is undirected, there is also a path from #j#
  to #i#). Show how to test if $G$ is connected in $O(#n#+#m#)$ time.
\end{exc}

\begin{exc}
  \index{connected components}%
  Let $G$ be an undirected graph.  A \emph{connected-component labelling}
  of $G$ partitions the vertices of $G$ into maximal sets, each of which
  forms a connected subgraph.  Show how to compute a connected component
  labelling of $G$ in $O(#n#+#m#)$ time.
\end{exc}

\begin{exc}
  \index{spanning forest}%
  Let $G$ be an undirected graph.  A \emph{spanning forest} of $G$ is a
  collection of trees, one per component, whose edges are edges of $G$
  and whose vertices contain all vertices of $G$.  Show how to compute
  a spanning forest of of $G$ in $O(#n#+#m#)$ time.
\end{exc}

\begin{exc}
  \index{strongly-connected graph}%
  \index{graph!strongly-connected}%
  We say that a graph $G$ is \emph{strongly-connected} if, for every
  pair of vertices #i# and #j# in $G$, there is a path from $#i#$ to
  $#j#$. Show how to test if $G$ is strongly-connected in $O(#n#+#m#)$
  time.
\end{exc}

\begin{exc}
  Given a graph $G=(V,E)$ and some special vertex $#r#\in V$, show how
  to compute the length of the shortest path from $#r#$ to #i# for every
  vertex $#i#\in V$.
\end{exc}

\begin{exc}
  Give a (simple) example where the #dfs(g,r)# code visits the nodes of a
  graph in an order that is different from that of the #dfs2(g,r)# code.
  Write a version of #dfs2(g,r)# that always visits nodes in exactly
  the same order as #dfs(g,r)#.  (Hint: Just start tracing the execution
  of each algorithm on some graph where #r# is the source of more than
  1 edge.)
\end{exc}

\begin{exc}
  \index{universal sink}%
  \index{celebrity|see{universal sink}}%
  A \emph{universal sink} in a graph $G$ is a vertex that is the target
  of $#n#-1$ edges and the source of no edges.\footnote{A universal sink,
  #v#, is also sometimes called a \emph{celebrity}: Everyone in the room
  recognizes #v#, but #v# doesn't recognize anyone else in the room.}
  Design and implement an algorithm that tests if a graph $G$, represented
  as an #AdjacencyMatrix#, has a universal sink.  Your algorithm should
  run in $O(#n#)$ time.
\end{exc}



