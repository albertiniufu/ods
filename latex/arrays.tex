\chapter{Listas Baseadas em Array}
\chaplabel{arrays}

Neste capítulo, iremos estudar implementações das interfaces #List# e #Queue#,
onde os dados são armazenados em um array chamado de \emph{backing array} ou \emph{array de apoio}.
\index{backing array}%
\index{array de apoio}%
A tabela a seguir resume o tempo de execução de operações para estruturas de dados apresentadas nestes capítulo:
\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}
Estruturas de dados que funcionam armazenando em um único array têm muitas vantagens e limitações em comum:
\index{arrays}%
\begin{itemize}
  \item Arrays permitem acesso em tempo constante a qualquer valor no array.
  Isso é o que permite #get(i)# e #set(i,x)# rodarem em tempo constante.

  \item Arrays não são muito dinâmicos. Adicionar ou remover um elemento perto do meio de uma lista significa que um grande número de elemento no array precisam
ser deslocados para abrir espaço para o elemento recentemente adicionado ou
preencher a lacuna criada pelo elemento removido. Essa é a razão pela qual as operações 
  #add(i,x)# e #remove(i)# tem tempos de execução que dependem de 
   #n# e #i#.

  \item Arrays não podem expandir ou encolher por si só. Quando o número de elementos na
    estrutura de dados excede o tamanho do array de apoio, um novo array precisa
    ser alocado e os dados do antigo array precisa ser copiado 
    no novo array. Essa é uma operação cara.
\end{itemize}
Um terceiro ponto é importante. Os tempos de execução citados na tabela
acima não incluem o custo associado com expandir ou encolher o array de apoio.
Veremos que, se não gerenciado com cuidado, o custo de expandir ou encolher o array de apoio não aumenta muito o custo de uma operação \emph{média}.
Mais precisamente, se iniciarmos com um estrutura de dados vazia e 
realizarmos qualquer sequência de $m$ operações #add(i,x)# ou #remove(i)#
, então o custo total de expandir e encolher o array de apoio, sobre a sequência inteira de $m$ operações é $O(m)$. Embora algumas operações individuais sejam mais caras, o custo amortizado, quando dividido por todas as $m$ operações, é somente $O(1)$ por operação.

\cpponly{
Neste capítulo, e ao longo deste livro, seria conveniente ter arrays que guardam seus tamanhos. Os arrays típicos do C++ não fazem isso, então definimos uma classe, #array#, que registra seu tamanho. A implementação dessa classe direta. Ela é feita como um array C++ padrão, #a#, e um inteiro, #length#:
}
\cppimport{ods/array.a.length}
\cpponly{
O tamanho de um #array# é especificado no momento de criação:
}
\cppimport{ods/array.array(len)}
\cpponly{Os eleementos de um array podem ser indexados:}
\cppimport{ods/array.operator[]}
\cpponly{Finalmente, quando um array é atribuído para outro, ocorre apenas uma manipulação de ponteiros que leva um tempo constante:}
\cppimport{ods/array.operator=}

\section{#ArrayStack#: Operações de Stack Rápida Usando um Array}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%
Um 
#ArrayStack# implementa a interface lista usando um array #a#, chamado de 
\emph{array de apoio}. O elemento da lista com índice #i# é armazenado
em #a[i]#.  Na maior parte do tempo, #a# é maior que o estritamente necessário,  
então um inteiro 
#n# é usado para registrar o número de elementos realmente armazenados em #a#. 
Dessa maneira, os elementos da lista são guardados em 
#a[0]#,\ldots,#a[n-1]# e, sempre, $#a.length# \ge #n#$.

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{O Básico}

Acessar e modificar os elementos de uma 
#ArrayStack# usando #get(i)# e #set(i,x)# é trivial. 
Após realizar as verificações de limites necessárias, simplesmente retornamos ou atribuímos, respectivamente, #a[i]#.

\codeimport{ods/ArrayStack.get(i).set(i,x)}

As operações de adicionar e remover elementos de um 
 #ArrayStack#
estão ilustradas em 
 \figref{arraystack}.  Para implementar a operação #add(i,x)#,
primeiro verificamos se #a# está cheio. Caso positivo, chamamos o método
#resize()# para aumentar o tamanho de #a#. Como #resize()#
é implementado será discutido depois. Por ora, é suficiente 
saber que, após uma chamada para #resize()#, temos certeza que $#a.length#
> #n#$.  
Com isso resolvido, agora nós deslocamos os elementos
$#a[i]#,\ldots,#a[n-1]#$ para uma posição à direita para
abrir espaço para #x#, atribuir
#a[i]# igual a #x#, e incrementar #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption[Adicionando a um ArrayStack]{Uma sequência de operações #add(i,x)# e #remove(i)# em um
  #ArrayStack#.  Flechas denotam elementos sendo copiados. Operações que
  resultam em uma chamada para 
  #resize()# são marcados em um arterisco.}
  \figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
Se ignorarmos o custo de uma potencial chamada a
#resize()#, então o cusot da operação 
#add(i,x)# é proporcional ao número de elementos que temos que deslocar para
abrir espaço para 
 #x#.  Portanto o custo dessa operação 
(ignorando o custo de redimensionar #a#) é $O(#n#-#i#)$.

Implementar a operação
#remove(i)# é similar. Desloca-se os elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda por uma posição (sobrescrevendo #a[i]#) 
e decrementar o valor de  
 #n#. Após fazer isso, verificamos se #n# muito menor 
 que #a.length# ao verificar se $#a.length# \ge 3#n#$. 
Caso positivo, então chamamos #resize()# para reduzir o tamanho de #a#.

\codeimport{ods/ArrayStack.remove(i)}
% TODO: Add shifting figure
Ao ignorar o custo do método #resize()#, o custo de uma operação #remove(i)#
é proporcional ao número de elementos que deslocamos, que é $O(#n#-#i#)$.

\subsection{Expansão e Redução}

O método 
 #resize()# razoavelmente direto; ele aloca um novo array
#b# de tamanho $2#n#$ e copia os #n# elementos de #a# nas primeiras 
#n# posições em #b#, e então atribui #a# em #b#. Então, após isso faz uma chamada a #resize()#, $#a.length# = 2#n#$.

\codeimport{ods/ArrayStack.resize()}

Analisar o custo real da operação 
#resize()# é fácil.
Ela aloca um array 
 #b# de tamanho $2#n#$ e copia os #n# elementos de #a# em 
#b#. Isso leva $O(#n#)$ de tempo.

A análise de tempo de execução da seção anterior ignorou o custo de chamadas a
#resize()#.  Nesta seção analisaremos esse custo usando uma técnica chamada de 
\emph{análise amortizada}.  Essa técnica não tenta determinar o custo de 
redimensionar o array durante cada operação 
#add(i,x)# e  #remove(i)#.  Em vez disso, ela considera o custo de todas as chamadas a 
#resize()# durante a sequência de $m$ chamadas a #add(i,x)# ou #remove(i)#.
Em particular, mostraremos que:

\begin{lem}\lemlabel{arraystack-amortized}
  Se um 
   #ArrayStack# vazio é criado e qualquer sequência de $m\ge 1$ chamadas a  
  #add(i,x)# e #remove(i)# são executadas, então o tempo total gasto durante
  todas as chamadas a 
 #resize()# é $O(m)$.
\end{lem}

\begin{proof}
  Nós iremos mostra que em qualquer momento que
 #resize()# é chamada, o número de chamadas a 
  #add# ou #remove# desde a última chamada a #resize()# é pelo menos 
  $#n#/2-1$.  Portanto, se $#n#_i$ denota o valor de #n# durante a 
  $i$-ésima chamada a #resize()# e $r$ denota o número de chamadas a 
  #resize()#, então o número total de chamadas a #add(i,x)# ou
  #remove(i)# é pelo menos 
  \[
     \sum_{i=1}^{r} (#n#_i/2-1) \le m  \enspace ,
  \]
  o que é equivalente a
  \[
    \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace .
  \]
  Por outro lado, o tempo total gasto durante todas as chamadas a  #resize()# é 
  \[
     \sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace ,
  \]
  uma vez que 
  $r$ não é maior que $m$.  O que resta é mostrar que o número de chamadas a 
   #add(i,x)# ou #remove(i)# entre a $(i-1)$-ésima
  e a $i$-ésima chamada a #resize()# é de pelo menos $#n#_i/2$.

  Há dois casos a considerar. No primeiro caso, 
 #resize()# está sendo chamado por 
#add(i,x)# pois o array de apoio #a# está cheio, i.e.,
  $#a.length# = #n#=#n#_i$.  Considere a chamada anterior a #resize()#:
  após essa chamada prévia, o tamanho de 
 #a# era #a.length#, mas o número de elementos guardados em #a# 
  era no máximo $#a.length#/2=#n#_i/2$.
  Mas agora o número de elementos guardados em 
 #a# é $#n#_i=#a.length#$, então deve ter havido pelo menos
$#n#_i/2$ chamadas a #add(i,x)# desde a chamada anterior a 
   #resize()#.
  % TODO: Add figure
  
  O segundo caso ocorre quando 
 #resize()# está sendo chamado por 
  #remove(i)# porque $#a.length# \ge 3#n#=3#n#_i$.  Novamente, após a
  chamada anterior a
 #resize()# o número de elementos guardados em #a# era pelo menos 
   $#a.length/2#-1$.\footnote{O ${}-1$ nessa fórmula inclui o caso especial que ocorre quando 
   $#n#=0$ e $#a.length# = 1$.} Agora há 
  $#n#_i\le#a.length#/3$ elementos guardados em #a#.  Portanto, o número de 
  operações #remove(i)# desde a última chamada a #resize()# é pelo menos 
  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1\enspace .
  \end{align*}
Nos dois casos, o número de chamadas a 
 #add(i,x)# ou #remove(i)# que ocorrem 
 entre 
   $(i-1)$-ésima chamada a #resize()# e a $i$-ésima chamada a 
  #resize()# é pelo menos $#n#_i/2-1$, conforme exigido para completar a prova.
\end{proof}

\subsection{Resumo}

O teorema a seguir resume o desempenho de uma #ArrayStack#:

\begin{thm}\thmlabel{arraystack}
  Uma
   #ArrayStack# implementa a interface #List#. Ignorando o custo de chamadas a 
  #resize()#, uma #ArrayStack# aceita as operações
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e
    \item #add(i,x)# e #remove(i)# em $O(1+#n#-#i#)$ tempo por operação.
  \end{itemize}
  Além disso, ao comerçarmos com um 
 #ArrayStack# vazio e realizarmos qualquer sequência de $m$ operações 
   #add(i,x)# e #remove(i)# resulta em um total de
   $O(m)$ tempo gasto durante todas as chamadas a #resize()#.
\end{thm}

O #ArrayStack# é um jeito eficiente de implementar a #Stack#.
Em especial, podemos implementar 
 #push(x)# como #add(n,x)# e #pop()#
como #remove(n-1)# e nesse caso essas operações rodarão $O(1)$
de tempo amortizado.

\section{#FastArrayStack#: Uma ArrayStack otimizada}
\seclabel{fastarraystack}

\index{FastArrayStack@#FastArrayStack#}%
Muito do trabalho feito por uma 
 #ArrayStack# envolver o deslocamento (por 
#add(i,x)# e #remove(i)#) e cópias (pelo #resize()#) de dados.
\notpcode{Nas implementações mostradas acima, isso era feito usando laços #for#.}%
\pcodeonly{Em uma implementação naive, isso seria feito usando laços #for#.}
Acontece que muitos ambientes de programação tem funções específicas que são muito 
eficientes em copiar e mover blocos de dados.
Na linguagem C,
existem as funções #memcpy(d,s,n)# e #memmove(d,s,n)#. 
Na linguagem C++ existe o algoritmo #std::copy(a0,a1,b)#.
Em Java, existe o 
método #System.arraycopy(s,i,d,j,n)#.
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\cppimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
\javaimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

Essas funções são em geral altamente otimizadas e podem usar até mesmo de 
instruções de máquina especiais que podem fazer esse copiar muito mais rápido 
que poderíamos usando um laço #for#.
Embora o uso dessas funções não diminuam o tempo de execução assintoticamente falando,
pode ser uma otimização que vale a pena.

\pcodeonly{Nas nossas implementações em C++ e Java, o uso de funções de cópia rápida de arrays
}
\notpcode{Nas implementações em \lang\ aqui, o uso do nativo \javaonly{#System.arraycopy(s,i,d,j,n)#}\cpponly{#std::copy(a0,a1,b)#}}
resultaram em um fator de speedups (aceleração) entre 2 e 3, dependendo dos tipos de operações realizadas.
O resultados podem variar de acordo com o caso.

\section{#ArrayQueue#: Uma Queue Baseada Em Array}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%

Nesta seção, apresentamos a estrutura de dados 
 #ArrayQueue#, que implementa uma queue do tipo 
 FIFO (first-in-first-out, primeiro-que-chega-primeiro-que-sai);
 elementos são removidos (usando a operação 
#remove()#) da queue na mesma ordem em que são adicionados 
(usando a operação #add(x)#).

Note uma 
#ArrayStack# é uma escolha ruim para uma implementação de uma 
queue do tipo FIFO. Não é uma boa escolha porque precisamos escolher um fim da lista ao qual adicionaremos elementos e então remover elementos do outro lado. 
Uma das duas operações precisa trabalhar na cabeça da lista, o que envolve chamar
#add(i,x)# ou #remove(i)# com um valor de $#i#=0$.
Isso resulta em um tempo de execução proporcional a #n#.

Para obter uma implementação de queue eficiente baseada em array,
primeiro observamos que o problema seria fácil se tivéssemos um array #a# infinito.
Poderíamos manter um índice
 #j# que guarda qual é o próximo elemento a remover e um inteiro
#n# que conta o número de elementos na queue.
Os elementos da queue sempre seriam guardados em 
\[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace . \]
Inicialmente, ambos #j# e #n# receberiam o valor 0. 
Para adicionar um elemento, teríamos que colocá-lo em #a[j+n]# e incrementar #n#.
Para remover um elemento, o removeríamos de 
 #a[j]#, incrementando #j#, e 
decrementando #n#.

É claro, o problema com essa solução é que ela requer um array infinito.
Um 
#ArrayQueue# simula isso ao usar um array finito #a#
e \emph{aritmética modular}.
\index{aritmética modular}%
Esse é o tipo de aritmética usada quando estamos falando sobre a hora do dia.
Por exemplo, 10:00 mais cinco horas resulta em 3:00. Formalmente, dizemos que
\[
    10 + 5 = 15 \equiv 3 \pmod{12} \enspace .
\]
Lemos a última parte dessa equação da forma ``15 é congruente a 3 módulo 12.''
Podemos também tratar 
 $\bmod$ como um operador binário, tal que 
\[
   15 \bmod 12 = 3 \enspace .
\]

De modo mais geral, para um inteiro
$a$ e um inteiro positivo $m$, $a \bmod m$
é único inteiro 
 $r\in\{0,\ldots,m-1\}$ tal que $a = r + km$ para algum inteiro $k$. 
Informalmente, o valor $r$ é o resto obtido quando dividimos $a$ por $m$.
\pcodeonly{Em muitas linguagens de programação, incluindo C, C++ e Java, o operador mod é representado
usando o símbolo \%.} 
\notpcode{Em muitas linguagens de programação incluindo 
\javaonly{Java}\cpponly{C++}, o operador $\bmod$ é representado 
usando o símbolo
 #%# symbol.\footnote{Isso às vezes é chamado de 
operador mod com \emph{morte cerebral}, pois não implementa corretamente
o operador matemático mod quando o primeiro argumento é negativo.}}

Aritmética modular é útil para simular um array infinito 
uma vez que 
$#i#\bmod #a.length#$ sempre resulta em um valor no intervalo
$0,\ldots,#a.length-1#$.  Usando aritmética modular podemos guardar
os elementos da queue em posições do array
\[ #a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\enspace. \]
Desse jeito trata-se o array 
 #a# como um \emph{array circular}
\index{array circular}%
\index{circular!array}%
no qual os índices do array maiores que
$#a.length#-1$ ``dão a volta'' ao começo do array. 
% TODO: figure

A única coisa que falta considerar é cuidar que o número de elementos no
 #ArrayQueue# não ultrapasse o tamanho de #a#.

\codeimport{ods/ArrayQueue.a.j.n}

A sequência de operações  
#add(x)# e #remove()# em um #ArrayQueue# é
ilustrado na \figref{arrayqueue}.  Para implementar #add(x)#, primeiro 
verificamos se 
 #a# está cheio e, se necessário, chamamos #resize()# para aumentar o tamanho de  
#a#.  Em seguida, guardamos #x# em
#a[(j+n)%a.length]# e incrementamos #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption[Adicionar e remover de um ArrayQueue]{Uma sequência de operações #add(x)# e #remove(i)# em um 
  #ArrayQueue#.  Flechas denotam elementos sendo copiados. Operações que resultam em uma chamada a
 #resize()# são marcadas com um asterisco.}
  \figlabel{arrayqueue}
\end{figure}

\codeimport{ods/ArrayQueue.add(x)}

Para implementar 
#remove()#, primeiro guardamos #a[j]# para que reutilizá-lo depois. 
A seguir, decrementamos #n# e incrementamos #j# (módulo #a.length#)
ao atribuir
$#j#=(#j#+1)\bmod #a.length#$.  Finalmente, retornamos o valor guardado de
#a[j]#. Se necessário, podemos chamar  #resize()# para diminuir o tamanho de  #a#.

\codeimport{ods/ArrayQueue.remove()}

Finalmente, a operação  
#resize()# é muito similar à operação #resize()#
do #ArrayStack#. Ela aloca um novo array, #b#, de tamanho $2#n#$
e copia 
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
em 
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
e atribui $#j#=0$.

\codeimport{ods/ArrayQueue.resize()}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados #ArrayQueue#:

\begin{thm}
Um #ArrayQueue# implementa a interface (FIFO) #Queue#. Ignorando o custo das chamadas a 
#resize()#, uma #ArrayQueue# aceita as operações 
#add(x)# e #remove()# em tempo $O(1)$ por operação. 
Além disso, ao começar com um #ArrayQueue# vazio, qualquer sequência de $m$
operações #add(i,x)# e #remove(i)# resulta em um total de $O(m)$ tempo gasto
durante todas as chamadas a #resize()#.
\end{thm}

%TODO: Discuss the use of bitwise-and as a replacement for the mod operator

\section{#ArrayDeque#: Operações Rápidas para Deque Usando um Array}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
O #ArrayQueue# da seção anterior é uma estrutura de daos para 
representar a sequência que nos permite eficientemente adicionar a
um lado da sequência e remover do outro.

A estrutura de dados #ArrayDeque# permite a edição e remoção eficiente em ambos lados.
Essa estrutura implementa a interface 
 #List# ao usar a mesma técnica de array circular
usada para representar um #ArrayQueue#.

\codeimport{ods/ArrayDeque.a.j.n}

As operações #get(i)# e #set(i,x)# em um #ArrayDeque# são simples
. Elas obtém ou atribui a um elemento do array $#a[#{#(j+i)#\bmod
#a.length#}#]#$.

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

A implementação de 
 #add(i,x)# é um pouco mais interessante. Como sempre, primeiro 
 verificamos se 
 #a# está cheio e, se necessário, chamados 
#resize()# para redimensionar #a#.  Lembre-se que queremos que essa operação
seja rápida quando 
#i# for pequeno (perto de 0) ou quando #i# é grande (perto de 
#n#).  Portanto, verificamos se $#i#<#n#/2$.  Caso positiov, deslocamos os
elementos $#a[0]#,\ldots,#a[i-1]#$ à esquerda por uma posição.  Caso contrário, 
($#i#\ge#n#/2$), deslocamos os elementos $#a[i]#,\ldots,#a[n-1]#$ à direito por uma posição 
. Veja \figref{arraydeque} para uma ilustração das operações 
#add(i,x)# e #remove(x)# em um #ArrayDeque#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption[Adição e remoção de ArrayDeque]{Uma sequência de operações #add(i,x)# e #remove(i)# em um 
  #ArrayDeque#.  Flechas denotam elementos sendo copiados.}
  \figlabel{arraydeque}
\end{figure}


\codeimport{ods/ArrayDeque.add(i,x)}

Ao deslocar dessa maneira, nós garantimos que #add(i,x)# nunca tem que deslocar mais de 
 $\min\{ #i#, #n#-#i# \}$ elementos.  Então, o tempo de execução da operação 
#add(i,x)# (ignorando o custo de uma operação #resize()#
) é $O(1+\min\{#i#,#n#-#i#\})$.

A implementação da operação  #remove(i)# é similar.  Ela ou desloca elementos
$#a[0]#,\ldots,#a[i-1]#$ à direita uma posição ou desloca elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda uma posição dependendo se 
$#i#<#n#/2$.  Novamente, isso significa que #remove(i)# nunca gasta mais de 
$O(1+\min\{#i#,#n#-#i#\})$ tempo para deslocar elementos.

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados
 #ArrayDeque#:
\begin{thm}\thmlabel{arraydeque}
  Uma #ArrayDeque# implementa a interface #List#.  Ignorando o custo de chamadas 
  a #resize()#, um #ArrayDeque# aceita as operações 
  \begin{itemize}
    \item #get(i)# e  #set(i,x)# em tempo $O(1)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(1+\min\{#i#,#n#-#i#\})$ 
          por operação.
  \end{itemize}
  Além disso, começar com um 
 #ArrayDeque# vazio, realizar qualquer sequência de $m$ operações 
  #add(i,x)# e #remove(i)# resulta em um
  total de tempo $O(m)$ gasto durante todas as chamadas a #resize()#.
\end{thm}

\section{#DualArrayDeque#: Construção de um Deque a Partir de Duas Stacks}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%
A seguir, apresentamos uma estrutura de dados, o 
 #DualArrayDeque# que atinge os mesmos desempenhos 
que um #ArrayDeque# ao usar 
duas #ArrayStack#s.  Embora o desempenho assintótico do
#DualArrayDeque# não é melhor que do #ArrayDeque#, ainda vale estudá-lo 
,pois oferece um bom exemplo de como fazer uma estrutura de dados sofisticada pela combinação de duas estruturas de dados mais simples.

Um #DualArrayDeque# representa uma lista usando duas #ArrayStack#s.  Relembre que um 
#ArrayStack# é rápido quando as operações dele modificam elementos perto do final.
Uma #DualArrayDeque# posiciona duas #ArrayStack#s, chamadas de #frontal#
and #traseira#, de modo complementar de tal forma que as operações são rápidas em ambas as direções.

\codeimport{ods/DualArrayDeque.front.back}

Um
 #DualArrayDeque# não guarda explicitamente o número, #n#,
 de elementos contidos. Ele não precisa, pois ele contém
$#n#=#front.size()# + #back.size()#$ elementos. De qualquer forma, ao
analisar o 
#DualArrayDeque# iremos usar ainda o #n# para denotar o número de 
elementos nele.

\codeimport{ods/DualArrayDeque.size()}

A #ArrayStack# #front# guarda os elementos da lista cujos índices são 
$0,\ldots,#front.size()#-1$, mas guarda-os em ordem reversa.
O #ArrayStack# #back# contém elementos da lista com índices 
em $#front.size()#,\ldots,#size()#-1$ na ordem normal. Desse jeito, 
#get(i)# e #set(i,x)# traduzem-se em chamadas apropriadas para #get(i)#
ou #set(i,x)# e ambos #front# ou #back#, que levam tempo $O(1)$ time por operação.

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

Note que se um índice
 $#i#<#front.size()#$, então ele corresponde ao elemento 
de #front# na posição $#front.size()#-#i#-1$, pois
elementos de #front# são guardados em ordem inversa.

A adição e remoção de elementos de uma #DualArrayDeque# é ilustrado em 
\figref{dualarraydeque}.  A operação #add(i,x)# manipula #front#
ou #back#, conforme apropriado:

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dualarraydeque}
  \end{center}
  \caption[Adição e remoção em um DualArrayDeque]{Uma sequência de operações #add(i,x)# e #remove(i)# em um 
  #DualArrayDeque#.  Flechas denotam elementos sendo copiados.  Operações que resultam em um
  rebalanceamento por #balance()# são marcados com um asterisco.}
  \figlabel{dualarraydeque}
\end{figure}

\codeimport{ods/DualArrayDeque.add(i,x)}

O método 
#add(i,x)# realiza balanceamento das duas  #ArrayStack#s
#front# e #back#, ao chamar o método #balance()#.
A implementação de 
#balance()# é descrita a seguir, mas no memento é suficiente 
saber que #balance()# garantes que, a não ser que $#size()#<2$,
#front.size()# e #back.size()# não diferem por mais de um fator 
de 3.  Em particular, $3\cdot#front.size()# \ge #back.size()#$ e
$3\cdot#back.size()# \ge #front.size()#$.

A seguir, nós analisamos o custo de 
 #add(i,x)#, ignorando o custo de chamadas 
#balance()#. Se $#i#<#front.size()#$, então #add(i,x)# é implementada 
pela chamada a
 $#front.add(front.size()-i-1,x)#$.  Como #front# é uma 
#ArrayStack#, o custo disso é 
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(#i#+1) \enspace .
  \eqlabel{das-front}
\end{equation}
Por outro lado, se 
 $#i#\ge#front.size()#$, então #add(i,x)# é 
implementado como $#back.add(i-front.size(),x)#$.  O custo disso é 
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(#n#-#i#+1) \enspace .
  \eqlabel{das-back}
\end{equation}

Note que o primeiro caso 
 \myeqref{das-front} ocorre quando $#i#<#n#/4$.
 O segundo caso
 \myeqref{das-back} ocorre quando $#i#\ge 3#n#/4$.  Quando
$#n#/4\le#i#<3#n#/4$, não temos certeza se a operação afeta 
#front# ou #back#, mas nos dois casos, a operação leva 
$O(#n#)=O(#i#)=O(#n#-#i#)$ de tempo, pois $#i#\ge #n#/4$ e $#n#-#i#>
#n#/4$.  Resumindo a situação, temos 
\[
     \mbox{Tempo de execução } #add(i,x)# \le 
          \begin{cases}
            O(1+ #i#) & \mbox{se $#i#< #n#/4$} \\
            O(#n#) & \mbox{se $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{se $#i# \ge 3#n#/4$}
          \end{cases}
\]
Então, o tempo de operação de 
 #add(i,x)#, se ignorarmos o custo à chamada 
#balance()#, é $O(1+\min\{#i#, #n#-#i#\})$.

A operação #remove(i)# e sua análise lembra a análise de #add(i,x)#.


\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{Balanceamento}

Finalmente, passamos à operação 
 #balance()# usada por #add(i,x)#
e #remove(i)#.  Essa operação garante que nem #front# nem #back#
se tornem grandes demais (ou pequenos demais). 
Ela garante que, ao menos que haja menos de dois elementos, o 
 #front# e o #back# contém $#n#/4$ elementos cada. 
Se esse não for o caso, então ele move elementos entre elas 
de tal forma que #front# e #back# contêm exatamente $\lfloor#n#/2\rfloor$ elementos
e $\lceil#n#/2\rceil$ elementos, respectivamente.

\codeimport{ods/DualArrayDeque.balance()}

Aqui há pouco para analisar. Se a operação 
#balance()# faz rebalanceamento 
, então ela move $O(#n#)$ elementos e isso leva $O(#n#)$ de tempo.
Isso é ruim, pois
 #balance()# é chamada a cada operação
#add(i,x)# e #remove(i)#.  Porém, o lema a seguir mostra que
na média, #balance()# somente gasta um tempo constante por operação.

\begin{lem}\lemlabel{dualarraydeque-amortized}
  Se uma 
  #DualArrayDeque# vazia é criada e qualquer sequência de chamadas $m\ge 1$ a
 #add(i,x)# e #remove(i)# são realizadas, então o tempo total gasto 
 durante todas as chamadas a 
 #balance()# é $O(m)$.
\end{lem}

\begin{proof}
  Iremos mostrar que, se 
 #balance()# é forçada a deslocar elementos, então o número de operações 
  #add(i,x)# e #remove(i)# desde a última vez que elementos foram deslocador por 
   #balance()# é pelo menos $#n#/2-1$.
   Como na prova de 
 \lemref{arraystack-amortized}, é suficiente provar que o tempo total gasto por 
   #balance()# é $O(m)$.

   Iremos realizar nossa análise usando uma técnica conhecida como o \emph{método do potencial}.
  \index{potencial}%
  \index{método do potential}%
  Definimos o \emph{potencial}, $\Phi$, do 
  #DualArrayDeque# como a diferença em tamanho entre #front# e #back#:
  \[  \Phi = |#front.size()# - #back.size()#| \enspace . \]
  A propriedade interessante desse potencial é que uma chamada a 
 #add(i,x)#
  ou #remove(i)# que não faz nenhum balanceamento pode aumentar o potencial em até 1.

  Observe que, imediatamente depois de uma chamada a #balance()# que desloca elementos 
  , o potencial , $\Phi_0$, é no máximo 1, pois
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace .\]

  Considere a situação no momento exatamente anterior a uma chamada #balance()# que 
  desloca elementos e suponha, sem perda de generalidade, que 
 #balance()#
está deslocando elementos porque $3#front.size()# < #back.size()#$.
Note que, nesse caso
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  Além disso, o potencial nesse momento é 
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  Portanto, o número de chamadas a 
 #add(i,x)# ou #remove(i)# desde a última vez que 
   #balance()# deslocou elementos é pelo menos $\Phi_1-\Phi_0
  > #n#/2-1$. Isso completa a prova.
\end{proof}

\subsection{Resumo}

O teorma a seguir resume as propriedades de uma #DualArrayDeque#:

\begin{thm}\thmlabel{dualarraydeque}
  Uma
   #DualArrayDeque# implementa a interface #List#. Ignorando o custo de chamadas a 
   #resize()# e #balance()#, uma #DualArrayDeque#
  aceita as operações 
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(1+\min\{#i#,#n#-#i#\})$ 
          por operação.
  \end{itemize}
  Além disso, ao começar com uma 
#DualArrayDeque# vazia, qualquer sequência de $m$ operações
  #add(i,x)# e #remove(i)# resulta em um total de tempo $O(m)$
  durante todas as chamadas a #resize()# e #balance()#.
\end{thm}


\section{#RootishArrayStack#: Uma Stack Array Eficiente No Uso de Espaço}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%
Uma das desvantagem de todas as estruturas de dados anteriores neste capítulo
é que, porque elas guardam os dados um array ou dois e evitam redimensionar
esses arrays com frequência, os arrays frequentemente não estão muito cheios.
Por exemplo, imediatamente após uma operação 
 #resize()# em uma #ArrayStack#,
o array de apoio #a# tem somente metade do espaço em uso.
E pior, às vezes somente um terço de #a# contém dados.

Nesta seção, distimos a estrutura de dados 
#RootishArrayStack#, que resolve o problema de espaço desperdiçado.
A #RootishArrayStack# guarda 
#n# elementos usando arrays de tamanho $O(\sqrt{#n#})$.
Nesses arrays, no máximo 
$O(\sqrt{#n#})$ posições do array estão vazias a qualquer momento.
Todo o restante do array está usando para guardar dados. Portanto,
essas estruturas de dados gastam até 
 $O(\sqrt{#n#})$ de espaço ao guardar #n#
elementos.

Uma #RootishArrayStack# guarda seus elementos em uma lista de #r#
arrays chamados de \emph{blocos} que são numerados $0,1,\ldots,#r#-1$.
Ver \figref{rootisharraystack}.  O bloco $b$ contém $b+1$ elementos.
Então, todos os 
 #r# blocos contém um total de
\[
  1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
elementos. A fórmula acima pode ser obtida conforme mostrado em \figref{gauss}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
  \end{center}
  \caption[Adição e remoção em uma RootishArrayStack]{Uma sequÊncia de operações #add(i,x)# e #remove(i)# em uma 
  #RootishArrayStack#.  Flechas denotam elementos sendo copiados. }
  \figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/gauss}
  \end{center}
  \caption{O número de quadrados brancos é $1+2+3+\cdots+#r#$.  O número de quadrados sombreados é o mesmo.
  Juntos, os quadrados brancos e sombreados compõem um retângulo consistindo de 
   $#r#(#r#+1)$ quadrados.}
  \figlabel{gauss}
\end{figure}

Como podemos esperar, os elementos da lista estão dispostos em ordem dentro dos blocos.
O elemento da lista com índice 0 é guardado no bloco 0, 
elementos com índices 1 e 2 são guardados no bloco 1, elementos
com índices 3, 4 e 5 são guardados no bloco 2 e assim por diante.
O principal problema que temos para resolver é o de determinar, dado um índice
 $#i#$,
 qual bloco contém 
 #i# e também o índice correspondente a #i# naquele bloco.

Determinar o índice de #i# no bloco acaba sendo fácil.
Se o índice 
#i# está no bloco #b#, então o número de elementos nos blocos 
$0,\ldots,#b#-1$ é $#b#(#b#+1)/2$.  Portanto, #i# é guardado na posição 
\[
     #j# = #i# - #b#(#b#+1)/2
\]
dentro do bloco #b#.  Um pouco mais desafiador é o problema de determinar o valor de 
#b#.  O número de elementos que tem índices menores que ou iguais a 
#i# é $#i#+1$.  Por outro lado, o número de elementos nos blocos 
$0,\ldots,b$ é $(#b#+1)(#b#+2)/2$.  Portanto, #b# é o menor inteiro tal que 

\[
    (#b#+1)(#b#+2)/2 \ge #i#+1 \enspace .
\]
Podemos reescrever essa equação da forma
\[
    #b#^2 + 3#b# - 2#i# \ge  0 \enspace .
\]
A equação quadrática correspondente 
 $#b#^2 + 3#b# - 2#i# =  0$ tem duas soluções
: $#b#=(-3 + \sqrt{9+8#i#}) / 2$ e $#b#=(-3 - \sqrt{9+8#i#}) / 2$.
A segunda solução não faz sentido na nossa aplicação pois é um valor negativo.
Portanto, obtemos a solução
$#b# = (-3 +
\sqrt{9+8i}) / 2$.  Em geral, essa solução não é um inteiro, mas retornando à nossa desigualdade, queremos o menor inteiro 
 $#b#$ tal que
$#b# \ge (-3 + \sqrt{9+8i}) / 2$.  Isso é simplesmente
\[
   #b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace .
\]

\codeimport{ods/RootishArrayStack.i2b(i)}

With this out of the way, the #get(i)# and #set(i,x)# methods are straightforward.  We first compute the appropriate block #b# and the appropriate index #j# within the block and then perform the appropriate operation:

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

Se usarmos quaisquer estruturas de dados neste capítulo para representar a lista de 
#blocks#, então #get(i)# e #set(i,x)# irão rodar em tempo constante.

O método #add(i,x)# irá, a esta altura, parecer familiar.  Primeiro verificamos 
se nossa estrutura de dados está cheia ao ver se o número de blocos 
, #r#, é tal que $#r#(#r#+1)/2 = #n#$. Caso positivo, chamamos #grow()#
para adicionar outro bloco. Com isso feito, deslocamos elementos com índices
$#i#,\ldots,#n#-1$ à direita por uma posição para abrir espaço para o novo elemento com índice 
 #i#:

\codeimport{ods/RootishArrayStack.add(i,x)}

O método #grow()# faz o que esperamos dele. Ele adiciona um novo bloco à estrutura de dados: 

\codeimport{ods/RootishArrayStack.grow()}

Ignorando o custo da operação 
 #grow()#, o custo da operação #add(i,x)# é dominado pelo custo de deslocamento e é portanto
$O(1+#n#-#i#)$, como um #ArrayStack#.

A operação #remove(i)# é similar a #add(i,x)#.  Ela desloca os elementos com índices 
$#i#+1,\ldots,#n#$ à esquerda por uma posição e então, se há mais de um bloco vazio, ela chama o método 
#shrink()# para remover todos menos um dos blocos não usados:

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

Novamente, ignorando o custo da operação 
 #shrink()#, o custo de uma operação 
#remove(i)# é dominado pelo custo de deslocamento e portanto
$O(#n#-#i#)$.

\subsection{Análise de expandir e encolher}

A análise acima de 
 #add(i,x)# e #remove(i)# não leva em conta o custo de 
#grow()# e #shrink()#.  Note que, diferentemente da operação
#ArrayStack.resize()#, #grow()# e #shrink()# não copiam nenhum dado 
. Elas simplesmente alocam ou liberam um array de tamanho #r#.  
Em alguns ambientes, isso leva apenas tempo constante, enquanto em outros, pode ser necessário tempo proporcional a #r#.

Note que, imediatamente após uma chamada a 
#grow()# ou #shrink()#, a situação é clara
. O bloco final está completamente vazio e todos os outros blocos estão completamente cheios. 
Outra chamada a 
   #grow()# ou #shrink()# não acontecerá até após pelo menos 
 $#r#-1$ elementos tenham sido adicionados ou removidos.
 Portanto, mesmo se 
#grow()# e #shrink()# leve tempo $O(#r#)$, esse custo pode ser amortizado sobre pelo menos 
$#r#-1$ operações #add(i,x)# ou #remove(i)#
fazendo com que o custo amortizado de 
 #grow()# e #shrink()# seja
$O(1)$ por operação.

\subsection{Uso de Espaço}
\seclabel{rootishspaceusage}

A seguir, analisaremos a quantidade de espaço extra usada por uma 
 #RootishArrayStack#.

Em particular, queremos contar qualquer espaço usado por uma #RootishArrayStack# que não é um elemento de array usado no momento para guardar um elemento da lista. Chamamos todo esse espaço de \emph{espaço desperdiçado}.
\index{espaço desperdiçado}%

% TODO continuar
The #remove(i)# operation ensures that a #RootishArrayStack# never has
more than two blocks that are not completely full.  The number of blocks,
#r#, used by a #RootishArrayStack# that stores #n# elements therefore
satisfies
\[
    (#r#-2)(#r#-1)/2 \le #n# \enspace .
\]
Again, using the quadratic equation on this gives
\[
   #r# \le \frac{1}{2}\left(3+\sqrt{8#n#+1}\right) = O(\sqrt{#n#}) \enspace .
\]
The last two blocks have sizes #r# and #r-1#, so the space wasted by these
two blocks is at most $2#r#-1 = O(\sqrt{#n#})$.  If we store the blocks
in (for example) an #ArrayStack#, then the amount of space wasted by the
#List# that stores those #r# blocks is also $O(#r#)=O(\sqrt{#n#})$.  The
other space needed for storing #n# and other accounting information is $O(1)$.
Therefore, the total amount of wasted space in a #RootishArrayStack#
is $O(\sqrt{#n#})$.

Next, we argue that this space usage is optimal for any data structure
that starts out empty and can support the addition of one item at
a time. More precisely, we will show that, at some point during the
addition of #n# items, the data structure is wasting at least in $\sqrt{#n#}$ space (though it may be only wasted for a moment).

Suppose we start with an empty data structure and we add #n# items one
at a time.  At the end of this process, all #n# items are stored in
the structure and distributed among a collection of #r# memory blocks.
If $#r#\ge \sqrt{#n#}$, then the data structure must be using #r#
pointers (or references) to keep track of these #r# blocks, and these
pointers are wasted space.  On the other hand, if $#r# < \sqrt{#n#}$
then, by the pigeonhole principle, some block must have size at
least $#n#/#r# > \sqrt{#n#}$.  Consider the moment at which this block
was first allocated.  Immediately after it was allocated, this block
was empty, and was therefore wasting $\sqrt{#n#}$ space.  Therefore,
at some point in time during the insertion of #n# elements, the data
structure was wasting $\sqrt{#n#}$ space.

\subsection{Summary}

The following theorem summarizes our discussion of the #RootishArrayStack#
data structure:

\begin{thm}\thmlabel{rootisharraystack}
  A #RootishArrayStack# implements the #List# interface.  Ignoring the cost of
  calls to #grow()# and #shrink()#, a #RootishArrayStack# supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+#n#-#i#)$ time per operation.
  \end{itemize}
  Furthermore, beginning with an empty #RootishArrayStack#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(m)$
  time spent during all calls to #grow()# and #shrink()#.

  The space (measured in words)\footnote{Recall \secref{model} for a
  discussion of how memory is measured.} used by a #RootishArrayStack#
  that stores #n# elements is $#n# +O(\sqrt{#n#})$.
\end{thm}

\notpcode{

\subsection{Computing Square Roots}

\index{square roots}%
A reader who has had some exposure to models of computation may notice
that the #RootishArrayStack#, as described above, does not fit into
the usual word-RAM model of computation (\secref{model}) because it
requires taking square roots.  The square root operation is generally
not considered a basic operation and is therefore not usually part of
the word-RAM model.

In this section, we show that the square root operation can be
implemented efficiently.  In particular, we show that for any integer
$#x#\in\{0,\ldots,#n#\}$,  $\lfloor\sqrt{#x#}\rfloor$ can be computed
in constant-time, after $O(\sqrt{#n#})$ preprocessing that creates two
arrays of length $O(\sqrt{#n#})$.  The following lemma shows that we
can reduce the problem of computing the square root of #x# to the square
root of a related value #x'#.

\begin{lem}\lemlabel{root}
Let $#x#\ge 1$ and let $#x'#=#x#-a$, where $0\le a\le\sqrt{#x#}$.  Then
   $\sqrt{x'} \ge \sqrt{#x#}-1$.
\end{lem}

\begin{proof}
It suffices to show that
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
Square both sides of this inequality to get
\[
 #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
and gather terms to get 
\[
 \sqrt{#x#} \ge 1
\]
which is clearly true for any $#x#\ge 1$.
\end{proof}

Start by restricting the problem a little, and assume that $2^{#r#} \le
#x# < 2^{#r#+1}$, so that $\lfloor\log #x#\rfloor=#r#$, i.e., #x# is an
integer having $#r#+1$ bits in its binary representation.  We can take
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$.  Now, #x'# satisfies
the conditions of \lemref{root}, so $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Furthermore, #x'# has all of its lower-order $\lfloor #r#/2\rfloor$ bits
equal to 0, so there are only
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
possible values of #x'#.  This means that we can use an array, #sqrttab#,
that stores the value of $\lfloor\sqrt{#x'#}\rfloor$ for each possible
value of #x'#.  A little more precisely, we have
\[
   #sqrttab#[i] 
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
In this way, $#sqrttab#[i]$ is within 2 of $\sqrt{#x#}$ for all
$#x#\in\{i2^{\lfloor #r#/2\rfloor},\ldots,(i+1)2^{\lfloor #r#/2\rfloor}-1\}$.
Stated another way, the array entry 
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ is either equal to
$\lfloor\sqrt{#x#}\rfloor$,
$\lfloor\sqrt{#x#}\rfloor-1$, or
$\lfloor\sqrt{#x#}\rfloor-2$.  From #s# we can determine the value
of $\lfloor\sqrt{#x#}\rfloor$ by
incrementing #s# until 
$(#s#+1)^2 > #x#$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x,r)}
\cppimport{ods/FastSqrt.sqrt(x,r)}
\notpcode{
Now, this only works for $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ and
#sqrttab# is a special table that only works for a particular value
of $#r#=\lfloor\log #x#\rfloor$.  To overcome this, we could compute
$\lfloor\log #n#\rfloor$ different #sqrttab# arrays, one for each possible
value of $\lfloor\log #x#\rfloor$. The sizes of these tables form an exponential sequence whose largest value is at most $4\sqrt{#n#}$, so the total size of all tables is $O(\sqrt{#n#})$.

However, it turns out that more than one #sqrttab# array is unnecessary;
we only need one #sqrttab# array for the value $#r#=\lfloor\log
#n#\rfloor$.  Any value #x# with $\log#x#=#r'#<#r#$ can be \emph{upgraded}
by multiplying #x# by $2^{#r#-#r'#}$ and using the equation
\[
    \sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
The quantity $2^{#r#-#r#'}x$ is in the range
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ so we can look up its square root
in #sqrttab#.  The following code implements this idea to compute
$\lfloor\sqrt{#x#}\rfloor$ for all non-negative integers #x# in the
range $\{0,\ldots,2^{30}-1\}$ using an array, #sqrttab#, of size $2^{16}$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x)}
\cppimport{ods/FastSqrt.sqrt(x)}
\notpcode{
Something we have taken for granted thus far is the question of how
to compute
$#r#'=\lfloor\log#x#\rfloor$.  Again, this is a problem that can be solved
with an array, #logtab#, of size $2^{#r#/2}$.  In this case, the
code is particularly simple, since $\lfloor\log #x#\rfloor$ is just the
index of the most significant 1 bit in the binary representation of #x#.
This means that, for $#x#>2^{#r#/2}$, we can right-shift the bits of
#x# by $#r#/2$ positions before using it as an index into #logtab#.
The following code does this using an array #logtab# of size $2^{16}$ to compute
$\lfloor\log #x#\rfloor$ for all #x# in the range $\{1,\ldots,2^{32}-1\}$.
} % notpcode
\javaimport{ods/FastSqrt.log(x)}
\cppimport{ods/FastSqrt.log(x)}
\notpcode{
Finally, for completeness, we include the following code that initializes #logtab# and #sqrttab#:
} % notpcode
\javaimport{ods/FastSqrt.inittabs()}
\cppimport{ods/FastSqrt.inittabs()}
\notpcode{
To summarize, the computations done by the #i2b(i)# method can be
implemented in constant time on the word-RAM using $O(\sqrt{n})$ extra
memory to store the #sqrttab# and #logtab# arrays.  These arrays can be
rebuilt when #n# increases or decreases by a factor of two, and the cost
of this rebuilding can be amortized over the number of #add(i,x)# and
#remove(i)# operations that caused the change in #n# in the same way that
the cost of #resize()# is analyzed in the #ArrayStack# implementation.
} % notpcode

\section{Discussion and Exercises}

Most of the data structures described in this chapter are folklore. They
can be found in implementations dating back over 30 years.  For example,
implementations of stacks, queues, and deques, which generalize easily
to the #ArrayStack#, #ArrayQueue# and #ArrayDeque# structures described
here, are discussed by Knuth \cite[Section~2.2.2]{k97v1}.

Brodnik \etal\ \cite{bcdms99} seem to have been the first to describe
the #RootishArrayStack# and prove a $\sqrt{n}$ lower-bound like that
in \secref{rootishspaceusage}.  They also present a different structure
that uses a more sophisticated choice of block sizes in order to avoid
computing square roots in the #i2b(i)# method.  Within their scheme,
the block containing #i# is block $\lfloor\log (#i#+1)\rfloor$, which
is simply the index of the leading 1 bit in the binary representation
of $#i#+1$.  Some computer architectures provide an instruction for
computing the index of the leading 1-bit in an integer. \javaonly{In
Java, the #Integer# class provides a method #numberOfLeadingZeros(i)#
from which one can easily compute $\lfloor\log (#i#+1)\rfloor$.}

A structure related to the #RootishArrayStack# is the two-level
\emph{tiered-vector} of Goodrich and Kloss \cite{gk99}.
\index{tiered-vector}%
This structure
supports the #get(i,x)# and #set(i,x)# operations in constant time and
#add(i,x)# and #remove(i)# in $O(\sqrt{#n#})$ time.  These running times
are similar to what can be achieved with the more careful implementation
of a #RootishArrayStack# discussed in \excref{rootisharraystack-fast}.

\javaonly{
\begin{exc}
  In the #ArrayStack# implementation, after the first call to #remove(i)#,
  the backing array, #a#, contains $#n#+1$ non-#null# values despite
  the fact that the #ArrayStack# only contains #n# elements.  Where is
  the extra non-#null# value?  Discuss any consequences this non-#null#
  value might have on the Java Runtime Environment's memory manager.
  \index{Java Runtime Environment}%
  \index{memory manager}%
\end{exc}
}

\begin{exc}
  The #List# method #addAll(i,c)# inserts all elements of the #Collection#
  #c# into the list at position #i#.  (The #add(i,x)# method is a special
  case where $#c#=\{#x#\}$.)  Explain why, for the data structures
  in this chapter, it is not efficient to implement #addAll(i,c)# by
  repeated calls to #add(i,x)#.  Design and implement a more efficient
  implementation.
\end{exc}

\begin{exc}
  Design and implement a \emph{#RandomQueue#}.
  \index{RandomQueue@#RandomQueue#}%
  This is an implementation
  of the #Queue# interface in which the #remove()# operation removes
  an element that is chosen uniformly at random among all the elements
  currently in the queue.  (Think of a #RandomQueue# as a bag in which
  we can add elements or reach in and blindly remove some random element.)
  The #add(x)# and #remove()# operations in a #RandomQueue# should run
  in amortized constant time per operation.
\end{exc}

\begin{exc}
  Design and implement a #Treque# (triple-ended queue). 
  \index{Treque@#Treque#}%
  This is a #List#
  implementation in which #get(i)# and #set(i,x)# run in constant time
  and #add(i,x)# and #remove(i)# run in time
  \[
     O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace .
  \]
  In other words, modifications are fast if they are near either
  end or near the middle of the list.
\end{exc}

\begin{exc}
  Implement a method #rotate(a,r)# that ``rotates'' the array #a#
  so that #a[i]# moves to $#a#[(#i#+#r#)\bmod #a.length#]$, for all
  $#i#\in\{0,\ldots,#a.length#\}$.
\end{exc}

\begin{exc}
  Implement a method #rotate(r)# that ``rotates'' a #List# so that
  list item #i# becomes list item $(#i#+#r#)\bmod #n#$.  When run on
  an #ArrayDeque#, or a #DualArrayDeque#, #rotate(r)# should run in
  $O(1+\min\{#r#,#n#-#r#\})$ time.
\end{exc}

\begin{exc}
  \pcodeonly{This exercise is left out of the \lang\ edition.}
  \notpcode{
  Modify the #ArrayDeque# implementation so that the shifting
  done by #add(i,x)#, #remove(i)#, and #resize()# is done using
  the faster #System.arraycopy(s,i,d,j,n)# method.}
\end{exc}

\begin{exc}
  Modify the #ArrayDeque# implementation so that it does not use the
  #%# operator (which is expensive on some systems).  Instead, it
  should make use of the fact that, if #a.length# is a power of 2,
  then 
  \[  #k%a.length#=#k&(a.length-1)# \enspace .
  \]
  (Here, #&# is the bitwise-and operator.)
\end{exc}

\begin{exc}
  Design and implement a variant of #ArrayDeque# that does not do any
  modular arithmetic at all.  Instead, all the data sits in a consecutive
  block, in order, inside an array.  When the data overruns the beginning
  or the end of this array, a modified #rebuild()# operation is performed.
  The amortized cost of all operations should be the same as in an
  #ArrayDeque#.

  \noindent Hint: Getting this to work is really all about how you implement
  the #rebuild()# operation.  You would like #rebuild()# to put the data
  structure into a state where the data cannot run off either end until
  at least $#n#/2$ operations have been performed.

  Test the performance of your implementation against the #ArrayDeque#.
  Optimize your implementation (by using #System.arraycopy(a,i,b,i,n)#)
  and see if you can get it to outperform the #ArrayDeque# implementation.
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{#i#,#n#-#i#\})$ time.
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{\sqrt{#n#},#n#-#i#\})$
  time. (For an idea on how to do this, see \secref{selist}.)
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)# and
  #remove(i,x)# operations in $O(1+\min\{#i#,\sqrt {#n#},#n#-#i#\})$ time.
  (See \secref{selist} for ideas on how to achieve this.)
\end{exc}

\begin{exc}
  Design and implement a #CubishArrayStack#.
  \index{CubishArrayStack@#CubishArrayStack#}%
  This three level structure
  implements the #List# interface using $O(#n#^{2/3})$ wasted space.
  In this structure, #get(i)# and #set(i,x)# take constant time; while
  #add(i,x)# and #remove(i)# take $O(#n#^{1/3})$ amortized time.
\end{exc}


