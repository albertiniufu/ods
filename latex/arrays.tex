\chapter{Listas Baseadas em Array}
\chaplabel{arrays}

Neste capítulo, iremos estudar implementações das interfaces #List# e #Queue#,
onde os dados são armazenados em um array chamado de \emph{backing array} ou \emph{array de apoio}.
\index{backing array}%
\index{array de apoio}%
A tabela a seguir resume o tempo de execução de operações para estruturas de dados apresentadas nestes capítulo:
\newlength{\tabsep}
\setlength{\tabsep}{\itemsep}
\addtolength{\tabsep}{\parsep}
\addtolength{\tabsep}{-2pt}
\begin{center}
\vspace{\tabsep}
\begin{tabular}{|l|l|l|} \hline
 & #get(i)#/#set(i,x)# & #add(i,x)#/#remove(i)# \\ \hline
#ArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\
#ArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#DualArrayDeque# & $O(1)$ & $O(\min\{#i#,#n#-#i#\})$ \\
#RootishArrayStack# & $O(1)$ & $O(#n#-#i#)$ \\ \hline
\end{tabular}
\vspace{\tabsep}
\end{center}
Estruturas de dados que funcionam armazenando em um único array têm muitas vantagens e limitações em comum:
\index{arrays}%
\begin{itemize}
  \item Arrays permitem acesso em tempo constante a qualquer valor no array.
  Isso é o que permite #get(i)# e #set(i,x)# rodarem em tempo constante.

  \item Arrays não são muito dinâmicos. Adicionar ou remover um elemento perto do meio de uma lista significa que um grande número de elemento no array precisam
ser deslocados para abrir espaço para o elemento recentemente adicionado ou
preencher a lacuna criada pelo elemento removido. Essa é a razão pela qual as operações 
  #add(i,x)# e #remove(i)# tem tempos de execução que dependem de 
   #n# e #i#.

  \item Arrays não podem expandir ou encolher por si só. Quando o número de elementos na
    estrutura de dados excede o tamanho do array de apoio, um novo array precisa
    ser alocado e os dados do antigo array precisa ser copiado 
    no novo array. Essa é uma operação cara.
\end{itemize}
Um terceiro ponto é importante. Os tempos de execução citados na tabela
acima não incluem o custo associado com expandir ou encolher o array de apoio.
Veremos que, se não gerenciado com cuidado, o custo de expandir ou encolher o array de apoio não aumenta muito o custo de uma operação \emph{média}.
Mais precisamente, se iniciarmos com um estrutura de dados vazia e 
realizarmos qualquer sequência de $m$ operações #add(i,x)# ou #remove(i)#
, então o custo total de expandir e encolher o array de apoio, sobre a sequência inteira de $m$ operações é $O(m)$. Embora algumas operações individuais sejam mais caras, o custo amortizado, quando dividido por todas as $m$ operações, é somente $O(1)$ por operação.

\cpponly{
Neste capítulo, e ao longo deste livro, seria conveniente ter arrays que guardam seus tamanhos. Os arrays típicos do C++ não fazem isso, então definimos uma classe, #array#, que registra seu tamanho. A implementação dessa classe direta. Ela é feita como um array C++ padrão, #a#, e um inteiro, #length#:
}
\cppimport{ods/array.a.length}
\cpponly{
O tamanho de um #array# é especificado no momento de criação:
}
\cppimport{ods/array.array(len)}
\cpponly{Os eleementos de um array podem ser indexados:}
\cppimport{ods/array.operator[]}
\cpponly{Finalmente, quando um array é atribuído para outro, ocorre apenas uma manipulação de ponteiros que leva um tempo constante:}
\cppimport{ods/array.operator=}

\section{#ArrayStack#: Operações de Stack Rápida Usando um Array}
\seclabel{arraystack}

\index{ArrayStack@#ArrayStack#}%
Um 
#ArrayStack# implementa a interface lista usando um array #a#, chamado de 
\emph{array de apoio}. O elemento da lista com índice #i# é armazenado
em #a[i]#.  Na maior parte do tempo, #a# é maior que o estritamente necessário,  
então um inteiro 
#n# é usado para registrar o número de elementos realmente armazenados em #a#. 
Dessa maneira, os elementos da lista são guardados em 
#a[0]#,\ldots,#a[n-1]# e, sempre, $#a.length# \ge #n#$.

\codeimport{ods/ArrayStack.a.n.size()}

\subsection{O Básico}

Acessar e modificar os elementos de uma 
#ArrayStack# usando #get(i)# e #set(i,x)# é trivial. 
Após realizar as verificações de limites necessárias, simplesmente retornamos ou atribuímos, respectivamente, #a[i]#.

\codeimport{ods/ArrayStack.get(i).set(i,x)}

As operações de adicionar e remover elementos de um 
 #ArrayStack#
estão ilustradas em 
 \figref{arraystack}.  Para implementar a operação #add(i,x)#,
primeiro verificamos se #a# está cheio. Caso positivo, chamamos o método
#resize()# para aumentar o tamanho de #a#. Como #resize()#
é implementado será discutido depois. Por ora, é suficiente 
saber que, após uma chamada para #resize()#, temos certeza que $#a.length#
> #n#$.  
Com isso resolvido, agora nós deslocamos os elementos
$#a[i]#,\ldots,#a[n-1]#$ para uma posição à direita para
abrir espaço para #x#, atribuir
#a[i]# igual a #x#, e incrementar #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraystack}
  \end{center}
  \caption[Adicionando a um ArrayStack]{Uma sequência de operações #add(i,x)# e #remove(i)# em um
  #ArrayStack#.  Flechas denotam elementos sendo copiados. Operações que
  resultam em uma chamada para 
  #resize()# são marcados em um arterisco.}
  \figlabel{arraystack}
\end{figure}

\codeimport{ods/ArrayStack.add(i,x)}
Se ignorarmos o custo de uma potencial chamada a
#resize()#, então o cusot da operação 
#add(i,x)# é proporcional ao número de elementos que temos que deslocar para
abrir espaço para 
 #x#.  Portanto o custo dessa operação 
(ignorando o custo de redimensionar #a#) é $O(#n#-#i#)$.

Implementar a operação
#remove(i)# é similar. Desloca-se os elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda por uma posição (sobrescrevendo #a[i]#) 
e decrementar o valor de  
 #n#. Após fazer isso, verificamos se #n# muito menor 
 que #a.length# ao verificar se $#a.length# \ge 3#n#$. 
Caso positivo, então chamamos #resize()# para reduzir o tamanho de #a#.

\codeimport{ods/ArrayStack.remove(i)}
% TODO: Add shifting figure
Ao ignorar o custo do método #resize()#, o custo de uma operação #remove(i)#
é proporcional ao número de elementos que deslocamos, que é $O(#n#-#i#)$.

\subsection{Expansão e Redução}

O método 
 #resize()# razoavelmente direto; ele aloca um novo array
#b# de tamanho $2#n#$ e copia os #n# elementos de #a# nas primeiras 
#n# posições em #b#, e então atribui #a# em #b#. Então, após isso faz uma chamada a #resize()#, $#a.length# = 2#n#$.

\codeimport{ods/ArrayStack.resize()}

Analisar o custo real da operação 
#resize()# é fácil.
Ela aloca um array 
 #b# de tamanho $2#n#$ e copia os #n# elementos de #a# em 
#b#. Isso leva $O(#n#)$ de tempo.

A análise de tempo de execução da seção anterior ignorou o custo de chamadas a
#resize()#.  Nesta seção analisaremos esse custo usando uma técnica chamada de 
\emph{análise amortizada}.  Essa técnica não tenta determinar o custo de 
redimensionar o array durante cada operação 
#add(i,x)# e  #remove(i)#.  Em vez disso, ela considera o custo de todas as chamadas a 
#resize()# durante a sequência de $m$ chamadas a #add(i,x)# ou #remove(i)#.
Em particular, mostraremos que:

\begin{lem}\lemlabel{arraystack-amortized}
  Se um 
   #ArrayStack# vazio é criado e qualquer sequência de $m\ge 1$ chamadas a  
  #add(i,x)# e #remove(i)# são executadas, então o tempo total gasto durante
  todas as chamadas a 
 #resize()# é $O(m)$.
\end{lem}

\begin{proof}
  Nós iremos mostra que em qualquer momento que
 #resize()# é chamada, o número de chamadas a 
  #add# ou #remove# desde a última chamada a #resize()# é pelo menos 
  $#n#/2-1$.  Portanto, se $#n#_i$ denota o valor de #n# durante a 
  $i$-ésima chamada a #resize()# e $r$ denota o número de chamadas a 
  #resize()#, então o número total de chamadas a #add(i,x)# ou
  #remove(i)# é pelo menos 
  \[
     \sum_{i=1}^{r} (#n#_i/2-1) \le m  \enspace ,
  \]
  o que é equivalente a
  \[
    \sum_{i=1}^{r} #n#_i \le 2m + 2r  \enspace .
  \]
  Por outro lado, o tempo total gasto durante todas as chamadas a  #resize()# é 
  \[
     \sum_{i=1}^{r} O(#n#_i) \le O(m+r) = O(m)  \enspace ,
  \]
  uma vez que 
  $r$ não é maior que $m$.  O que resta é mostrar que o número de chamadas a 
   #add(i,x)# ou #remove(i)# entre a $(i-1)$-ésima
  e a $i$-ésima chamada a #resize()# é de pelo menos $#n#_i/2$.

  Há dois casos a considerar. No primeiro caso, 
 #resize()# está sendo chamado por 
#add(i,x)# pois o array de apoio #a# está cheio, i.e.,
  $#a.length# = #n#=#n#_i$.  Considere a chamada anterior a #resize()#:
  após essa chamada prévia, o tamanho de 
 #a# era #a.length#, mas o número de elementos guardados em #a# 
  era no máximo $#a.length#/2=#n#_i/2$.
  Mas agora o número de elementos guardados em 
 #a# é $#n#_i=#a.length#$, então deve ter havido pelo menos
$#n#_i/2$ chamadas a #add(i,x)# desde a chamada anterior a 
   #resize()#.
  % TODO: Add figure
  
  O segundo caso ocorre quando 
 #resize()# está sendo chamado por 
  #remove(i)# porque $#a.length# \ge 3#n#=3#n#_i$.  Novamente, após a
  chamada anterior a
 #resize()# o número de elementos guardados em #a# era pelo menos 
   $#a.length/2#-1$.\footnote{O ${}-1$ nessa fórmula inclui o caso especial que ocorre quando 
   $#n#=0$ e $#a.length# = 1$.} Agora há 
  $#n#_i\le#a.length#/3$ elementos guardados em #a#.  Portanto, o número de 
  operações #remove(i)# desde a última chamada a #resize()# é pelo menos 
  \begin{align*}
      R & \ge #a.length#/2 - 1 - #a.length#/3 \\
        & = #a.length#/6 - 1 \\
        & = (#a.length#/3)/2 - 1 \\
        & \ge #n#_i/2 -1\enspace .
  \end{align*}
Nos dois casos, o número de chamadas a 
 #add(i,x)# ou #remove(i)# que ocorrem 
 entre 
   $(i-1)$-ésima chamada a #resize()# e a $i$-ésima chamada a 
  #resize()# é pelo menos $#n#_i/2-1$, conforme exigido para completar a prova.
\end{proof}

\subsection{Resumo}

O teorema a seguir resume o desempenho de uma #ArrayStack#:

\begin{thm}\thmlabel{arraystack}
  Uma
   #ArrayStack# implementa a interface #List#. Ignorando o custo de chamadas a 
  #resize()#, uma #ArrayStack# aceita as operações
  \begin{itemize}
    \item #get(i)# e #set(i,x)# em tempo $O(1)$ por operação; e
    \item #add(i,x)# e #remove(i)# em $O(1+#n#-#i#)$ tempo por operação.
  \end{itemize}
  Além disso, ao comerçarmos com um 
 #ArrayStack# vazio e realizarmos qualquer sequência de $m$ operações 
   #add(i,x)# e #remove(i)# resulta em um total de
   $O(m)$ tempo gasto durante todas as chamadas a #resize()#.
\end{thm}

O #ArrayStack# é um jeito eficiente de implementar a #Stack#.
Em especial, podemos implementar 
 #push(x)# como #add(n,x)# e #pop()#
como #remove(n-1)# e nesse caso essas operações rodarão $O(1)$
de tempo amortizado.

\section{#FastArrayStack#: Uma ArrayStack otimizada}
\seclabel{fastarraystack}

\index{FastArrayStack@#FastArrayStack#}%
Muito do trabalho feito por uma 
 #ArrayStack# envolver o deslocamento (por 
#add(i,x)# e #remove(i)#) e cópias (pelo #resize()#) de dados.
\notpcode{Nas implementações mostradas acima, isso era feito usando laços #for#.}%
\pcodeonly{Em uma implementação naive, isso seria feito usando laços #for#.}
Acontece que muitos ambientes de programação tem funções específicas que são muito 
eficientes em copiar e mover blocos de dados.
Na linguagem C,
existem as funções #memcpy(d,s,n)# e #memmove(d,s,n)#. 
Na linguagem C++ existe o algoritmo #std::copy(a0,a1,b)#.
Em Java, existe o 
método #System.arraycopy(s,i,d,j,n)#.
\index{memcpy@#memcpy(d,s,n)#}%
\index{std::copy@#std::copy(a0,a1,b)#}%
\index{System.arraycopy@#System.arraycopy(s,i,d,j,n)#}%

\cppimport{ods/FastArrayStack.add(i,x).remove(i).resize()}
\javaimport{ods/FastArrayStack.add(i,x).remove(i).resize()}

Essas funções são em geral altamente otimizadas e podem usar até mesmo de 
instruções de máquina especiais que podem fazer esse copiar muito mais rápido 
que poderíamos usando um laço #for#.
Embora o uso dessas funções não diminuam o tempo de execução assintoticamente falando,
pode ser uma otimização que vale a pena.

\pcodeonly{Nas nossas implementações em C++ e Java, o uso de funções de cópia rápida de arrays
}
\notpcode{Nas implementações em \lang\ aqui, o uso do nativo \javaonly{#System.arraycopy(s,i,d,j,n)#}\cpponly{#std::copy(a0,a1,b)#}}
resultaram em um fator de speedups (aceleração) entre 2 e 3, dependendo dos tipos de operações realizadas.
O resultados podem variar de acordo com o caso.

\section{#ArrayQueue#: Uma Queue Baseada Em Array}
\seclabel{arrayqueue}

\index{ArrayQueue@#ArrayQueue#}%

Nesta seção, apresentamos a estrutura de dados 
 #ArrayQueue#, que implementa uma queue do tipo 
 FIFO (first-in-first-out, primeiro-que-chega-primeiro-que-sai);
 elementos são removidos (usando a operação 
#remove()#) da queue na mesma ordem em que são adicionados 
(usando a operação #add(x)#).

Note uma 
#ArrayStack# é uma escolha ruim para uma implementação de uma 
queue do tipo FIFO. Não é uma boa escolha porque precisamos escolher um fim da lista ao qual adicionaremos elementos e então remover elementos do outro lado. 
Uma das duas operações precisa trabalhar na cabeça da lista, o que envolve chamar
#add(i,x)# ou #remove(i)# com um valor de $#i#=0$.
Isso resulta em um tempo de execução proporcional a #n#.

Para obter uma implementação de queue eficiente baseada em array,
primeiro observamos que o problema seria fácil se tivéssemos um array #a# infinito.
Poderíamos manter um índice
 #j# que guarda qual é o próximo elemento a remover e um inteiro
#n# que conta o número de elementos na queue.
Os elementos da queue sempre seriam guardados em 
\[ #a[j]#,#a[j+1]#,\ldots,#a[j+n-1]# \enspace . \]
Inicialmente, ambos #j# e #n# receberiam o valor 0. 
Para adicionar um elemento, teríamos que colocá-lo em #a[j+n]# e incrementar #n#.
Para remover um elemento, o removeríamos de 
 #a[j]#, incrementando #j#, e 
decrementando #n#.

É claro, o problema com essa solução é que ela requer um array infinito.
Um 
#ArrayQueue# simula isso ao usar um array finito #a#
e \emph{aritmética modular}.
\index{aritmética modular}%
Esse é o tipo de aritmética usada quando estamos falando sobre a hora do dia.
Por exemplo, 10:00 mais cinco horas resulta em 3:00. Formalmente, dizemos que
\[
    10 + 5 = 15 \equiv 3 \pmod{12} \enspace .
\]
Lemos a última parte dessa equação da forma ``15 é congruente a 3 módulo 12.''
Podemos também tratar 
 $\bmod$ como um operador binário, tal que 
\[
   15 \bmod 12 = 3 \enspace .
\]

De modo mais geral, para um inteiro
$a$ e um inteiro positivo $m$, $a \bmod m$
é único inteiro 
 $r\in\{0,\ldots,m-1\}$ tal que $a = r + km$ para algum inteiro $k$. 
Informalmente, o valor $r$ é o resto obtido quando dividimos $a$ por $m$.
\pcodeonly{Em muitas linguagens de programação, incluindo C, C++ e Java, o operador mod é representado
usando o símbolo \%.} 
\notpcode{Em muitas linguagens de programação incluindo 
\javaonly{Java}\cpponly{C++}, o operador $\bmod$ é representado 
usando o símbolo
 #%# symbol.\footnote{Isso às vezes é chamado de 
operador mod com \emph{morte cerebral}, pois não implementa corretamente
o operador matemático mod quando o primeiro argumento é negativo.}}

Aritmética modular é útil para simular um array infinito 
uma vez que 
$#i#\bmod #a.length#$ sempre resulta em um valor no intervalo
$0,\ldots,#a.length-1#$.  Usando aritmética modular podemos guardar
os elementos da queue em posições do array
\[ #a[j%a.length]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\enspace. \]
Desse jeito trata-se o array 
 #a# como um \emph{array circular}
\index{array circular}%
\index{circular!array}%
no qual os índices do array maiores que
$#a.length#-1$ ``dão a volta'' ao começo do array. 
% TODO: figure

A única coisa que falta considerar é cuidar que o número de elementos no
 #ArrayQueue# não ultrapasse o tamanho de #a#.

\codeimport{ods/ArrayQueue.a.j.n}

A sequência de operações  
#add(x)# e #remove()# em um #ArrayQueue# é
ilustrado na \figref{arrayqueue}.  Para implementar #add(x)#, primeiro 
verificamos se 
 #a# está cheio e, se necessário, chamamos #resize()# para aumentar o tamanho de  
#a#.  Em seguida, guardamos #x# em
#a[(j+n)%a.length]# e incrementamos #n#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arrayqueue}
  \end{center}
  \caption[Adicionar e remover de um ArrayQueue]{Uma sequência de operações #add(x)# e #remove(i)# em um 
  #ArrayQueue#.  Flechas denotam elementos sendo copiados. Operações que resultam em uma chamada a
 #resize()# são marcadas com um asterisco.}
  \figlabel{arrayqueue}
\end{figure}

\codeimport{ods/ArrayQueue.add(x)}

Para implementar 
#remove()#, primeiro guardamos #a[j]# para que reutilizá-lo depois. 
A seguir, decrementamos #n# e incrementamos #j# (módulo #a.length#)
ao atribuir
$#j#=(#j#+1)\bmod #a.length#$.  Finalmente, retornamos o valor guardado de
#a[j]#. Se necessário, podemos chamar  #resize()# para diminuir o tamanho de  #a#.

\codeimport{ods/ArrayQueue.remove()}

Finalmente, a operação  
#resize()# é muito similar à operação #resize()#
do #ArrayStack#. Ela aloca um novo array, #b#, de tamanho $2#n#$
e copia 
\[
   #a[j]#,#a[(j+1)%a.length]#,\ldots,#a[(j+n-1)%a.length]#
\]
em 
\[
   #b[0]#,#b[1]#,\ldots,#b[n-1]#
\]
e atribui $#j#=0$.

\codeimport{ods/ArrayQueue.resize()}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados #ArrayQueue#:

\begin{thm}
Um #ArrayQueue# implementa a interface (FIFO) #Queue#. Ignorando o custo das chamadas a 
#resize()#, uma #ArrayQueue# aceita as operações 
#add(x)# e #remove()# em tempo $O(1)$ por operação. 
Além disso, ao começar com um #ArrayQueue# vazio, qualquer sequência de $m$
operações #add(i,x)# e #remove(i)# resulta em um total de $O(m)$ tempo gasto
durante todas as chamadas a #resize()#.
\end{thm}

%TODO: Discuss the use of bitwise-and as a replacement for the mod operator

\section{#ArrayDeque#: Operações Rápidas para Deque Usando um Array}
\seclabel{arraydeque}

\index{ArrayDeque@#ArrayDeque#}%
O #ArrayQueue# da seção anterior é uma estrutura de daos para 
representar a sequência que nos permite eficientemente adicionar a
um lado da sequência e remover do outro.

A estrutura de dados #ArrayDeque# permite a edição e remoção eficiente em ambos lados.
Essa estrutura implementa a interface 
 #List# ao usar a mesma técnica de array circular
usada para representar um #ArrayQueue#.

\codeimport{ods/ArrayDeque.a.j.n}

As operações #get(i)# e #set(i,x)# em um #ArrayDeque# são simples
. Elas obtém ou atribui a um elemento do array $#a[#{#(j+i)#\bmod
#a.length#}#]#$.

\codeimport{ods/ArrayDeque.get(i).set(i,x)}

A implementação de 
 #add(i,x)# é um pouco mais interessante. Como sempre, primeiro 
 verificamos se 
 #a# está cheio e, se necessário, chamados 
#resize()# para redimensionar #a#.  Lembre-se que queremos que essa operação
seja rápida quando 
#i# for pequeno (perto de 0) ou quando #i# é grande (perto de 
#n#).  Portanto, verificamos se $#i#<#n#/2$.  Caso positiov, deslocamos os
elementos $#a[0]#,\ldots,#a[i-1]#$ à esquerda por uma posição.  Caso contrário, 
($#i#\ge#n#/2$), deslocamos os elementos $#a[i]#,\ldots,#a[n-1]#$ à direito por uma posição 
. Veja \figref{arraydeque} para uma ilustração das operações 
#add(i,x)# e #remove(x)# em um #ArrayDeque#.

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/arraydeque}
  \end{center}
  \caption[Adição e remoção de ArrayDeque]{Uma sequência de operações #add(i,x)# e #remove(i)# em um 
  #ArrayDeque#.  Flechas denotam elementos sendo copiados.}
  \figlabel{arraydeque}
\end{figure}


\codeimport{ods/ArrayDeque.add(i,x)}

Ao deslocar dessa maneira, nós garantimos que #add(i,x)# nunca tem que deslocar mais de 
 $\min\{ #i#, #n#-#i# \}$ elementos.  Então, o tempo de execução da operação 
#add(i,x)# (ignorando o custo de uma operação #resize()#
) é $O(1+\min\{#i#,#n#-#i#\})$.

A implementação da operação  #remove(i)# é similar.  Ela ou desloca elementos
$#a[0]#,\ldots,#a[i-1]#$ à direita uma posição ou desloca elementos 
$#a[i+1]#,\ldots,#a[n-1]#$ à esquerda uma posição dependendo se 
$#i#<#n#/2$.  Novamente, isso significa que #remove(i)# nunca gasta mais de 
$O(1+\min\{#i#,#n#-#i#\})$ tempo para deslocar elementos.

\codeimport{ods/ArrayDeque.remove(i)}

\subsection{Resumo}

O teorema a seguir resume o desempenho da estrutura de dados
 #ArrayDeque#:
\begin{thm}\thmlabel{arraydeque}
  Uma #ArrayDeque# implementa a interface #List#.  Ignorando o custo de chamadas 
  a #resize()#, um #ArrayDeque# aceita as operações 
  \begin{itemize}
    \item #get(i)# e  #set(i,x)# em tempo $O(1)$ por operação; e 
    \item #add(i,x)# e #remove(i)# em tempo $O(1+\min\{#i#,#n#-#i#\})$ 
          por operação.
  \end{itemize}
  Além disso, começar com um 
 #ArrayDeque# vazio, realizar qualquer sequência de $m$ operações 
  #add(i,x)# e #remove(i)# resulta em um
  total de tempo $O(m)$ gasto durante todas as chamadas a #resize()#.
\end{thm}

\section{#DualArrayDeque#: Construção de um Deque a Partir de Duas Stacks}
\seclabel{dualarraydeque}

\index{DualArrayDeque@#DualArrayDeque#}%
A seguir, apresentamos uma estrutura de dados, o 
 #DualArrayDeque# que atinge os mesmos desempenhos 
que um #ArrayDeque# ao usar 
duas #ArrayStack#s.  Embora o desempenho assintótico do
#DualArrayDeque# não é melhor que do #ArrayDeque#, ainda vale estudá-lo 
,pois oferece um bom exemplo de como fazer uma estrutura de dados sofisticada pela combinação de duas estruturas de dados mais simples.

Um #DualArrayDeque# representa uma lista usando duas #ArrayStack#s.  Relembre que um 
#ArrayStack# é rápido quando as operações dele modificam elementos perto do final.
Uma #DualArrayDeque# posiciona duas #ArrayStack#s, chamadas de #frontal#
and #traseira#, de modo complementar de tal forma que as operações são rápidas em ambas as direções.

\codeimport{ods/DualArrayDeque.front.back}

A #DualArrayDeque# does not explicitly store the number, #n#,
of elements it contains.  It doesn't need to, since it contains
$#n#=#front.size()# + #back.size()#$ elements.  Nevertheless, when
analyzing the #DualArrayDeque# we will still use #n# to denote the number
of elements it contains.

\codeimport{ods/DualArrayDeque.size()}

A #ArrayStack# #front# guarda os elementos da lista cujos índices são 
$0,\ldots,#front.size()#-1$, mas guarda-os em ordem reversa.
O #ArrayStack# #back# contém elementos da lista com índices 
em $#front.size()#,\ldots,#size()#-1$ na ordem normal. Desse jeito, 
#get(i)# e #set(i,x)# traduzem-se em chamadas apropriadas para #get(i)#
ou #set(i,x)# e ambos #front# ou #back#, que levam tempo $O(1)$ time por operação.

\codeimport{ods/DualArrayDeque.get(i).set(i,x)}

Note that if an index $#i#<#front.size()#$, then it corresponds to the
element of #front# at position $#front.size()#-#i#-1$, since the
elements of #front# are stored in reverse order.

Adding and removing elements from a #DualArrayDeque# is illustrated in
\figref{dualarraydeque}.  The #add(i,x)# operation manipulates either #front#
or #back#, as appropriate:

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/dualarraydeque}
  \end{center}
  \caption[Adding and removing in a DualArrayDeque]{A sequence of #add(i,x)# and #remove(i)# operations on a
  #DualArrayDeque#.  Arrows denote elements being copied.  Operations that
  result in a rebalancing by #balance()# are marked with an asterisk.}
  \figlabel{dualarraydeque}
\end{figure}



\codeimport{ods/DualArrayDeque.add(i,x)}

The #add(i,x)# method performs rebalancing of the two #ArrayStack#s
#front# and #back#, by calling the #balance()# method.  The
implementation of #balance()# is described below, but for now it is
sufficient to know that #balance()# ensures that, unless $#size()#<2$,
#front.size()# and #back.size()# do not differ by more than a factor
of 3.  In particular, $3\cdot#front.size()# \ge #back.size()#$ and
$3\cdot#back.size()# \ge #front.size()#$.

Next we analyze the cost of #add(i,x)#, ignoring the cost of calls to
#balance()#. If $#i#<#front.size()#$, then #add(i,x)# gets implemented
by the call to $#front.add(front.size()-i-1,x)#$.  Since #front# is an
#ArrayStack#, the cost of this is
\begin{equation}
  O(#front.size()#-(#front.size()#-#i#-1)+1) = O(#i#+1) \enspace .
  \eqlabel{das-front}
\end{equation}
On the other hand, if $#i#\ge#front.size()#$, then #add(i,x)# gets
implemented as $#back.add(i-front.size(),x)#$.  The cost of this is
\begin{equation}
  O(#back.size()#-(#i#-#front.size()#)+1) = O(#n#-#i#+1) \enspace .
  \eqlabel{das-back}
\end{equation}

Notice that the first case \myeqref{das-front} occurs when $#i#<#n#/4$.
The second case \myeqref{das-back} occurs when $#i#\ge 3#n#/4$.  When
$#n#/4\le#i#<3#n#/4$, we cannot be sure whether the operation affects
#front# or #back#, but in either case, the operation takes
$O(#n#)=O(#i#)=O(#n#-#i#)$ time, since $#i#\ge #n#/4$ and $#n#-#i#>
#n#/4$.  Summarizing the situation, we have
\[
     \mbox{Running time of } #add(i,x)# \le 
          \begin{cases}
            O(1+ #i#) & \mbox{if $#i#< #n#/4$} \\
            O(#n#) & \mbox{if $#n#/4 \le #i# < 3#n#/4$} \\
            O(1+#n#-#i#) & \mbox{if $#i# \ge 3#n#/4$}
          \end{cases}
\]
Thus, the running time of #add(i,x)#, if we ignore the cost of the call
to #balance()#, is $O(1+\min\{#i#, #n#-#i#\})$.

The #remove(i)# operation and its analysis resemble the #add(i,x)#
operation and analysis.

\codeimport{ods/DualArrayDeque.remove(i)}

\subsection{Balancing}

Finally, we turn to the #balance()# operation performed by #add(i,x)#
and #remove(i)#.  This operation ensures that neither #front# nor #back#
becomes too big (or too small).  It ensures that, unless there are fewer
than two elements, each of #front# and #back# contain at least $#n#/4$
elements. If this is not the case, then it moves elements between them
so that #front# and #back# contain exactly $\lfloor#n#/2\rfloor$ elements
and $\lceil#n#/2\rceil$ elements, respectively.

\codeimport{ods/DualArrayDeque.balance()}

Here there is little to analyze.  If the #balance()# operation does
rebalancing, then it moves $O(#n#)$ elements and this takes $O(#n#)$
time. This is bad, since #balance()# is called with each call to
#add(i,x)# and #remove(i)#.  However, the following lemma shows that, on
average, #balance()# only spends a constant amount of time per operation.

\begin{lem}\lemlabel{dualarraydeque-amortized}
  If an empty #DualArrayDeque# is created and any sequence of $m\ge 1$ calls
  to #add(i,x)# and #remove(i)# are performed, then the total time spent
  during all calls to #balance()# is $O(m)$.
\end{lem}

\begin{proof}
  We will show that, if #balance()# is forced to shift elements, then
  the number of #add(i,x)# and #remove(i)# operations since the last
  time any elements were shifted by #balance()# is at least $#n#/2-1$.
  As in the proof of \lemref{arraystack-amortized}, this is sufficient
  to prove that the total time spent by #balance()# is $O(m)$.

  We will perform our analysis using a technique knows as the
  \emph{potential method}.
  \index{potential}%
  \index{potential method}%
  Define the \emph{potential}, $\Phi$, of the
  #DualArrayDeque# as the difference in size between #front# and #back#:
  \[  \Phi = |#front.size()# - #back.size()#| \enspace . \]
  The interesting thing about this potential is that a call to #add(i,x)#
  or #remove(i)# that does not do any balancing can increase the potential
  by at most 1.

  Observe that, immediately after a call to #balance()# that shifts
  elements, the potential, $\Phi_0$, is at most 1, since
  \[ \Phi_0 = \left|\lfloor#n#/2\rfloor-\lceil#n#/2\rceil\right|\le 1  \enspace .\]

  Consider the situation immediately before a call to #balance()# that
  shifts elements and suppose, without loss of generality, that #balance()#
  is shifting elements because $3#front.size()# < #back.size()#$.
  Notice that, in this case,
  \begin{eqnarray*}
   #n# & = & #front.size()#+#back.size()# \\
       & < & #back.size()#/3+#back.size()# \\
       & = & \frac{4}{3}#back.size()#
  \end{eqnarray*}
  Furthermore, the potential at this point in time is
  \begin{eqnarray*}
  \Phi_1 & = & #back.size()# - #front.size()# \\
      &>& #back.size()# - #back.size()#/3 \\
      &=& \frac{2}{3}#back.size()# \\
      &>& \frac{2}{3}\times\frac{3}{4}#n# \\
      &=& #n#/2
  \end{eqnarray*}
  Therefore, the number of calls to #add(i,x)# or #remove(i)# since
  the last time #balance()# shifted elements is at least $\Phi_1-\Phi_0
  > #n#/2-1$. This completes the proof.
\end{proof}

\subsection{Summary}

The following theorem summarizes the properties of a #DualArrayDeque#:

\begin{thm}\thmlabel{dualarraydeque}
  A #DualArrayDeque# implements the #List# interface.  Ignoring the
  cost of calls to #resize()# and #balance()#, a #DualArrayDeque#
  supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+\min\{#i#,#n#-#i#\})$ time
          per operation.
  \end{itemize}
  Furthermore, beginning with an empty #DualArrayDeque#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(m)$
  time spent during all calls to #resize()# and #balance()#.
\end{thm}


\section{#RootishArrayStack#: A Space-Efficient Array Stack}
\seclabel{rootisharraystack}

\index{RootishArrayStack@#RootishArrayStack#}%
One of the drawbacks of all previous data structures in this chapter is
that, because they store their data in one or two arrays and they avoid
resizing these arrays too often, the arrays frequently are not very full.
For example, immediately after a #resize()# operation on an #ArrayStack#,
the backing array #a# is only half full.  Even worse, there are times
when only one third of #a# contains data.

In this section, we discuss the #RootishArrayStack# data structure,
that addresses the problem of wasted space.  The #RootishArrayStack#
stores #n# elements using $O(\sqrt{#n#})$ arrays.  In these arrays, at
most $O(\sqrt{#n#})$ array locations are unused at any time.  All
remaining array locations are used to store data.  Therefore, these
data structures waste at most $O(\sqrt{#n#})$ space when storing #n#
elements.

A #RootishArrayStack# stores its elements in a list of #r#
arrays called \emph{blocks} that are numbered $0,1,\ldots,#r#-1$.
See \figref{rootisharraystack}.  Block $b$ contains $b+1$ elements.
Therefore, all #r# blocks contain a total of
\[
  1+ 2+ 3+\cdots +#r# = #r#(#r#+1)/2
\]
elements.  The above formula can be obtained as shown in \figref{gauss}.

\begin{figure}
  \begin{center}
    \includegraphics[width=\ScaleIfNeeded]{figs/rootisharraystack}
  \end{center}
  \caption[Adding and removing in a RootishArrayStack]{A sequence of #add(i,x)# and #remove(i)# operations on a
  #RootishArrayStack#.  Arrows denote elements being copied. }
  \figlabel{rootisharraystack}
\end{figure}

\codeimport{ods/RootishArrayStack.blocks.n}

\begin{figure}
  \begin{center}
    \includegraphics[scale=0.90909]{figs/gauss}
  \end{center}
  \caption{The number of white squares is $1+2+3+\cdots+#r#$.  The number of
  shaded squares is the same.  Together the white and shaded squares make a
  rectangle consisting of $#r#(#r#+1)$ squares.}
  \figlabel{gauss}
\end{figure}

As we might expect, the elements of the list are laid out in order
within the blocks.  The list element with index 0 is stored in block 0,
elements with list indices 1 and 2 are stored in block 1, elements with
list indices 3, 4, and 5 are stored in block 2, and so on.  The main
problem we have to address is that of determining, given an index $#i#$,
which block contains #i# as well as the index corresponding to #i#
within that block.

Determining the index of #i# within its block turns out to be easy. If
index #i# is in block #b#, then the number of elements in blocks
$0,\ldots,#b#-1$ is $#b#(#b#+1)/2$.  Therefore, #i# is stored at location
\[
     #j# = #i# - #b#(#b#+1)/2
\]
within block #b#.  Somewhat more challenging is the problem of determining
the value of #b#.  The number of elements that have indices less than
or equal to #i# is $#i#+1$.  On the other hand, the number of elements
in blocks $0,\ldots,b$ is $(#b#+1)(#b#+2)/2$.  Therefore, #b# is the smallest
integer such that
\[
    (#b#+1)(#b#+2)/2 \ge #i#+1 \enspace .
\]
We can rewrite this equation as
\[
    #b#^2 + 3#b# - 2#i# \ge  0 \enspace .
\]
The corresponding quadratic equation $#b#^2 + 3#b# - 2#i# =  0$ has two
solutions: $#b#=(-3 + \sqrt{9+8#i#}) / 2$ and $#b#=(-3 - \sqrt{9+8#i#}) / 2$.
The second solution makes no sense in our application since it always
gives a negative value. Therefore, we obtain the solution $#b# = (-3 +
\sqrt{9+8i}) / 2$.  In general, this solution is not an integer, but
going back to our inequality, we want the smallest integer $#b#$ such that 
$#b# \ge (-3 + \sqrt{9+8i}) / 2$.  This is simply
\[
   #b# = \left\lceil(-3 + \sqrt{9+8i}) / 2\right\rceil \enspace .
\]

\codeimport{ods/RootishArrayStack.i2b(i)}

With this out of the way, the #get(i)# and #set(i,x)# methods are straightforward.  We first compute the appropriate block #b# and the appropriate index #j# within the block and then perform the appropriate operation:

\codeimport{ods/RootishArrayStack.get(i).set(i,x)}

If we use any of the data structures in this chapter for representing the #blocks# list, then #get(i)# and #set(i,x)# will each run in constant time.

The #add(i,x)# method will, by now, look familiar.  We first check
to see if our data structure is full, by checking if the number of
blocks, #r#, is such that $#r#(#r#+1)/2 = #n#$. If so, we call #grow()#
to add another block.  With this done, we shift elements with indices
$#i#,\ldots,#n#-1$ to the right by one position to make room for the
new element with index #i#:

\codeimport{ods/RootishArrayStack.add(i,x)}

The #grow()# method does what we expect. It adds a new block:

\codeimport{ods/RootishArrayStack.grow()}

Ignoring the cost of the #grow()# operation, the cost of an #add(i,x)#
operation is dominated by the cost of shifting and is therefore
$O(1+#n#-#i#)$, just like an #ArrayStack#.

The #remove(i)# operation is similar to #add(i,x)#.  It shifts the
elements with indices $#i#+1,\ldots,#n#$ left by one position and then,
if there is more than one empty block, it calls the #shrink()# method
to remove all but one of the unused blocks:

\codeimport{ods/RootishArrayStack.remove(i)}
\codeimport{ods/RootishArrayStack.shrink()}

Once again, ignoring the cost of the #shrink()# operation, the cost of
a #remove(i)# operation is dominated by the cost of shifting  and is
therefore $O(#n#-#i#)$.

\subsection{Analysis of Growing and Shrinking}

The above analysis of #add(i,x)# and #remove(i)# does not account for
the cost of #grow()# and #shrink()#.  Note that, unlike the
#ArrayStack.resize()# operation, #grow()# and #shrink()# do not copy
any data.  They only allocate or free an array of size #r#.  In
some environments, this takes only constant time, while in others, it
may require time proportional to #r#.

We note that, immediately after a call to #grow()# or #shrink()#, the
situation is clear. The final block is completely empty, and all other
blocks are completely full.  Another call to #grow()# or #shrink()# will
not happen until at least $#r#-1$ elements have been added or removed.
Therefore, even if #grow()# and #shrink()# take $O(#r#)$ time, this
cost can be amortized over at least $#r#-1$ #add(i,x)# or #remove(i)#
operations, so that the amortized cost of #grow()# and #shrink()# is
$O(1)$ per operation.

\subsection{Space Usage}
\seclabel{rootishspaceusage}

Next, we analyze the amount of extra space used by a #RootishArrayStack#.
In particular, we want to count any space used by a #RootishArrayStack# that is not an array element currently used to hold a list element.  We call all such space \emph{wasted space}.
\index{wasted space}%

The #remove(i)# operation ensures that a #RootishArrayStack# never has
more than two blocks that are not completely full.  The number of blocks,
#r#, used by a #RootishArrayStack# that stores #n# elements therefore
satisfies
\[
    (#r#-2)(#r#-1)/2 \le #n# \enspace .
\]
Again, using the quadratic equation on this gives
\[
   #r# \le \frac{1}{2}\left(3+\sqrt{8#n#+1}\right) = O(\sqrt{#n#}) \enspace .
\]
The last two blocks have sizes #r# and #r-1#, so the space wasted by these
two blocks is at most $2#r#-1 = O(\sqrt{#n#})$.  If we store the blocks
in (for example) an #ArrayStack#, then the amount of space wasted by the
#List# that stores those #r# blocks is also $O(#r#)=O(\sqrt{#n#})$.  The
other space needed for storing #n# and other accounting information is $O(1)$.
Therefore, the total amount of wasted space in a #RootishArrayStack#
is $O(\sqrt{#n#})$.

Next, we argue that this space usage is optimal for any data structure
that starts out empty and can support the addition of one item at
a time. More precisely, we will show that, at some point during the
addition of #n# items, the data structure is wasting at least in $\sqrt{#n#}$ space (though it may be only wasted for a moment).

Suppose we start with an empty data structure and we add #n# items one
at a time.  At the end of this process, all #n# items are stored in
the structure and distributed among a collection of #r# memory blocks.
If $#r#\ge \sqrt{#n#}$, then the data structure must be using #r#
pointers (or references) to keep track of these #r# blocks, and these
pointers are wasted space.  On the other hand, if $#r# < \sqrt{#n#}$
then, by the pigeonhole principle, some block must have size at
least $#n#/#r# > \sqrt{#n#}$.  Consider the moment at which this block
was first allocated.  Immediately after it was allocated, this block
was empty, and was therefore wasting $\sqrt{#n#}$ space.  Therefore,
at some point in time during the insertion of #n# elements, the data
structure was wasting $\sqrt{#n#}$ space.

\subsection{Summary}

The following theorem summarizes our discussion of the #RootishArrayStack#
data structure:

\begin{thm}\thmlabel{rootisharraystack}
  A #RootishArrayStack# implements the #List# interface.  Ignoring the cost of
  calls to #grow()# and #shrink()#, a #RootishArrayStack# supports the operations
  \begin{itemize}
    \item #get(i)# and #set(i,x)# in $O(1)$ time per operation; and
    \item #add(i,x)# and #remove(i)# in $O(1+#n#-#i#)$ time per operation.
  \end{itemize}
  Furthermore, beginning with an empty #RootishArrayStack#, any sequence of $m$
  #add(i,x)# and #remove(i)# operations results in a total of $O(m)$
  time spent during all calls to #grow()# and #shrink()#.

  The space (measured in words)\footnote{Recall \secref{model} for a
  discussion of how memory is measured.} used by a #RootishArrayStack#
  that stores #n# elements is $#n# +O(\sqrt{#n#})$.
\end{thm}

\notpcode{

\subsection{Computing Square Roots}

\index{square roots}%
A reader who has had some exposure to models of computation may notice
that the #RootishArrayStack#, as described above, does not fit into
the usual word-RAM model of computation (\secref{model}) because it
requires taking square roots.  The square root operation is generally
not considered a basic operation and is therefore not usually part of
the word-RAM model.

In this section, we show that the square root operation can be
implemented efficiently.  In particular, we show that for any integer
$#x#\in\{0,\ldots,#n#\}$,  $\lfloor\sqrt{#x#}\rfloor$ can be computed
in constant-time, after $O(\sqrt{#n#})$ preprocessing that creates two
arrays of length $O(\sqrt{#n#})$.  The following lemma shows that we
can reduce the problem of computing the square root of #x# to the square
root of a related value #x'#.

\begin{lem}\lemlabel{root}
Let $#x#\ge 1$ and let $#x'#=#x#-a$, where $0\le a\le\sqrt{#x#}$.  Then
   $\sqrt{x'} \ge \sqrt{#x#}-1$.
\end{lem}

\begin{proof}
It suffices to show that
\[
\sqrt{#x#-\sqrt{#x#}} \ge \sqrt{#x#}-1 \enspace .
\]
Square both sides of this inequality to get
\[
 #x#-\sqrt{#x#} \ge #x#-2\sqrt{#x#}+1
\]
and gather terms to get 
\[
 \sqrt{#x#} \ge 1
\]
which is clearly true for any $#x#\ge 1$.
\end{proof}

Start by restricting the problem a little, and assume that $2^{#r#} \le
#x# < 2^{#r#+1}$, so that $\lfloor\log #x#\rfloor=#r#$, i.e., #x# is an
integer having $#r#+1$ bits in its binary representation.  We can take
$#x'#=#x# - (#x#\bmod 2^{\lfloor r/2\rfloor})$.  Now, #x'# satisfies
the conditions of \lemref{root}, so $\sqrt{#x#}-\sqrt{#x'#} \le 1$.
Furthermore, #x'# has all of its lower-order $\lfloor #r#/2\rfloor$ bits
equal to 0, so there are only
\[
  2^{#r#+1-\lfloor #r#/2\rfloor} \le 4\cdot2^{#r#/2} \le 4\sqrt{#x#}
\]
possible values of #x'#.  This means that we can use an array, #sqrttab#,
that stores the value of $\lfloor\sqrt{#x'#}\rfloor$ for each possible
value of #x'#.  A little more precisely, we have
\[
   #sqrttab#[i] 
    = \left\lfloor
       \sqrt{i 2^{\lfloor #r#/2\rfloor}}
      \right\rfloor \enspace .
\]
In this way, $#sqrttab#[i]$ is within 2 of $\sqrt{#x#}$ for all
$#x#\in\{i2^{\lfloor #r#/2\rfloor},\ldots,(i+1)2^{\lfloor #r#/2\rfloor}-1\}$.
Stated another way, the array entry 
$#s#=#sqrttab#[#x##>>#\lfloor #r#/2\rfloor]$ is either equal to
$\lfloor\sqrt{#x#}\rfloor$,
$\lfloor\sqrt{#x#}\rfloor-1$, or
$\lfloor\sqrt{#x#}\rfloor-2$.  From #s# we can determine the value
of $\lfloor\sqrt{#x#}\rfloor$ by
incrementing #s# until 
$(#s#+1)^2 > #x#$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x,r)}
\cppimport{ods/FastSqrt.sqrt(x,r)}
\notpcode{
Now, this only works for $#x#\in\{2^{#r#},\ldots,2^{#r#+1}-1\}$ and
#sqrttab# is a special table that only works for a particular value
of $#r#=\lfloor\log #x#\rfloor$.  To overcome this, we could compute
$\lfloor\log #n#\rfloor$ different #sqrttab# arrays, one for each possible
value of $\lfloor\log #x#\rfloor$. The sizes of these tables form an exponential sequence whose largest value is at most $4\sqrt{#n#}$, so the total size of all tables is $O(\sqrt{#n#})$.

However, it turns out that more than one #sqrttab# array is unnecessary;
we only need one #sqrttab# array for the value $#r#=\lfloor\log
#n#\rfloor$.  Any value #x# with $\log#x#=#r'#<#r#$ can be \emph{upgraded}
by multiplying #x# by $2^{#r#-#r'#}$ and using the equation
\[
    \sqrt{2^{#r#-#r'#}x} = 2^{(#r#-#r#')/2}\sqrt{#x#} \enspace .
\]
The quantity $2^{#r#-#r#'}x$ is in the range
$\{2^{#r#},\ldots,2^{#r#+1}-1\}$ so we can look up its square root
in #sqrttab#.  The following code implements this idea to compute
$\lfloor\sqrt{#x#}\rfloor$ for all non-negative integers #x# in the
range $\{0,\ldots,2^{30}-1\}$ using an array, #sqrttab#, of size $2^{16}$.
} % notpcode
\javaimport{ods/FastSqrt.sqrt(x)}
\cppimport{ods/FastSqrt.sqrt(x)}
\notpcode{
Something we have taken for granted thus far is the question of how
to compute
$#r#'=\lfloor\log#x#\rfloor$.  Again, this is a problem that can be solved
with an array, #logtab#, of size $2^{#r#/2}$.  In this case, the
code is particularly simple, since $\lfloor\log #x#\rfloor$ is just the
index of the most significant 1 bit in the binary representation of #x#.
This means that, for $#x#>2^{#r#/2}$, we can right-shift the bits of
#x# by $#r#/2$ positions before using it as an index into #logtab#.
The following code does this using an array #logtab# of size $2^{16}$ to compute
$\lfloor\log #x#\rfloor$ for all #x# in the range $\{1,\ldots,2^{32}-1\}$.
} % notpcode
\javaimport{ods/FastSqrt.log(x)}
\cppimport{ods/FastSqrt.log(x)}
\notpcode{
Finally, for completeness, we include the following code that initializes #logtab# and #sqrttab#:
} % notpcode
\javaimport{ods/FastSqrt.inittabs()}
\cppimport{ods/FastSqrt.inittabs()}
\notpcode{
To summarize, the computations done by the #i2b(i)# method can be
implemented in constant time on the word-RAM using $O(\sqrt{n})$ extra
memory to store the #sqrttab# and #logtab# arrays.  These arrays can be
rebuilt when #n# increases or decreases by a factor of two, and the cost
of this rebuilding can be amortized over the number of #add(i,x)# and
#remove(i)# operations that caused the change in #n# in the same way that
the cost of #resize()# is analyzed in the #ArrayStack# implementation.
} % notpcode

\section{Discussion and Exercises}

Most of the data structures described in this chapter are folklore. They
can be found in implementations dating back over 30 years.  For example,
implementations of stacks, queues, and deques, which generalize easily
to the #ArrayStack#, #ArrayQueue# and #ArrayDeque# structures described
here, are discussed by Knuth \cite[Section~2.2.2]{k97v1}.

Brodnik \etal\ \cite{bcdms99} seem to have been the first to describe
the #RootishArrayStack# and prove a $\sqrt{n}$ lower-bound like that
in \secref{rootishspaceusage}.  They also present a different structure
that uses a more sophisticated choice of block sizes in order to avoid
computing square roots in the #i2b(i)# method.  Within their scheme,
the block containing #i# is block $\lfloor\log (#i#+1)\rfloor$, which
is simply the index of the leading 1 bit in the binary representation
of $#i#+1$.  Some computer architectures provide an instruction for
computing the index of the leading 1-bit in an integer. \javaonly{In
Java, the #Integer# class provides a method #numberOfLeadingZeros(i)#
from which one can easily compute $\lfloor\log (#i#+1)\rfloor$.}

A structure related to the #RootishArrayStack# is the two-level
\emph{tiered-vector} of Goodrich and Kloss \cite{gk99}.
\index{tiered-vector}%
This structure
supports the #get(i,x)# and #set(i,x)# operations in constant time and
#add(i,x)# and #remove(i)# in $O(\sqrt{#n#})$ time.  These running times
are similar to what can be achieved with the more careful implementation
of a #RootishArrayStack# discussed in \excref{rootisharraystack-fast}.

\javaonly{
\begin{exc}
  In the #ArrayStack# implementation, after the first call to #remove(i)#,
  the backing array, #a#, contains $#n#+1$ non-#null# values despite
  the fact that the #ArrayStack# only contains #n# elements.  Where is
  the extra non-#null# value?  Discuss any consequences this non-#null#
  value might have on the Java Runtime Environment's memory manager.
  \index{Java Runtime Environment}%
  \index{memory manager}%
\end{exc}
}

\begin{exc}
  The #List# method #addAll(i,c)# inserts all elements of the #Collection#
  #c# into the list at position #i#.  (The #add(i,x)# method is a special
  case where $#c#=\{#x#\}$.)  Explain why, for the data structures
  in this chapter, it is not efficient to implement #addAll(i,c)# by
  repeated calls to #add(i,x)#.  Design and implement a more efficient
  implementation.
\end{exc}

\begin{exc}
  Design and implement a \emph{#RandomQueue#}.
  \index{RandomQueue@#RandomQueue#}%
  This is an implementation
  of the #Queue# interface in which the #remove()# operation removes
  an element that is chosen uniformly at random among all the elements
  currently in the queue.  (Think of a #RandomQueue# as a bag in which
  we can add elements or reach in and blindly remove some random element.)
  The #add(x)# and #remove()# operations in a #RandomQueue# should run
  in amortized constant time per operation.
\end{exc}

\begin{exc}
  Design and implement a #Treque# (triple-ended queue). 
  \index{Treque@#Treque#}%
  This is a #List#
  implementation in which #get(i)# and #set(i,x)# run in constant time
  and #add(i,x)# and #remove(i)# run in time
  \[
     O(1+\min\{#i#, #n#-#i#, |#n#/2-#i#|\}) \enspace .
  \]
  In other words, modifications are fast if they are near either
  end or near the middle of the list.
\end{exc}

\begin{exc}
  Implement a method #rotate(a,r)# that ``rotates'' the array #a#
  so that #a[i]# moves to $#a#[(#i#+#r#)\bmod #a.length#]$, for all
  $#i#\in\{0,\ldots,#a.length#\}$.
\end{exc}

\begin{exc}
  Implement a method #rotate(r)# that ``rotates'' a #List# so that
  list item #i# becomes list item $(#i#+#r#)\bmod #n#$.  When run on
  an #ArrayDeque#, or a #DualArrayDeque#, #rotate(r)# should run in
  $O(1+\min\{#r#,#n#-#r#\})$ time.
\end{exc}

\begin{exc}
  \pcodeonly{This exercise is left out of the \lang\ edition.}
  \notpcode{
  Modify the #ArrayDeque# implementation so that the shifting
  done by #add(i,x)#, #remove(i)#, and #resize()# is done using
  the faster #System.arraycopy(s,i,d,j,n)# method.}
\end{exc}

\begin{exc}
  Modify the #ArrayDeque# implementation so that it does not use the
  #%# operator (which is expensive on some systems).  Instead, it
  should make use of the fact that, if #a.length# is a power of 2,
  then 
  \[  #k%a.length#=#k&(a.length-1)# \enspace .
  \]
  (Here, #&# is the bitwise-and operator.)
\end{exc}

\begin{exc}
  Design and implement a variant of #ArrayDeque# that does not do any
  modular arithmetic at all.  Instead, all the data sits in a consecutive
  block, in order, inside an array.  When the data overruns the beginning
  or the end of this array, a modified #rebuild()# operation is performed.
  The amortized cost of all operations should be the same as in an
  #ArrayDeque#.

  \noindent Hint: Getting this to work is really all about how you implement
  the #rebuild()# operation.  You would like #rebuild()# to put the data
  structure into a state where the data cannot run off either end until
  at least $#n#/2$ operations have been performed.

  Test the performance of your implementation against the #ArrayDeque#.
  Optimize your implementation (by using #System.arraycopy(a,i,b,i,n)#)
  and see if you can get it to outperform the #ArrayDeque# implementation.
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{#i#,#n#-#i#\})$ time.
\end{exc}

\begin{exc}\exclabel{rootisharraystack-fast}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)#
  and #remove(i,x)# operations in $O(1+\min\{\sqrt{#n#},#n#-#i#\})$
  time. (For an idea on how to do this, see \secref{selist}.)
\end{exc}

\begin{exc}
  Design and implement a version of a #RootishArrayStack# that has
  only $O(\sqrt{#n#})$ wasted space, but that can perform #add(i,x)# and
  #remove(i,x)# operations in $O(1+\min\{#i#,\sqrt {#n#},#n#-#i#\})$ time.
  (See \secref{selist} for ideas on how to achieve this.)
\end{exc}

\begin{exc}
  Design and implement a #CubishArrayStack#.
  \index{CubishArrayStack@#CubishArrayStack#}%
  This three level structure
  implements the #List# interface using $O(#n#^{2/3})$ wasted space.
  In this structure, #get(i)# and #set(i,x)# take constant time; while
  #add(i,x)# and #remove(i)# take $O(#n#^{1/3})$ amortized time.
\end{exc}


